[2022-07-16 09:49:02,657] INFO: {
    "abs_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/data/h36m/",
    "link_log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/log_last.log",
    "link_val_log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/val_last.log",
    "log_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log",
    "log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/log_2022_07_16_09_44_37.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 10,
        "h36m_input_length_dct": 10,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 10,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "siMLPe_my",
    "root_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/snapshot",
    "this_dir": "baseline_h36m",
    "use_relative_loss": true,
    "val_log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/val_2022_07_16_09_44_37.log",
    "weight_decay": 0.0001
}
[2022-07-16 09:49:22,757] INFO: Iter 100 Summary: 
[2022-07-16 09:49:22,774] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.08613236263394355
[2022-07-16 09:49:39,422] INFO: Iter 200 Summary: 
[2022-07-16 09:49:39,423] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07508872278034687
[2022-07-16 09:49:56,160] INFO: Iter 300 Summary: 
[2022-07-16 09:49:56,161] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07509162284433842
[2022-07-16 09:50:12,988] INFO: Iter 400 Summary: 
[2022-07-16 09:50:12,990] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07482885286211967
[2022-07-16 09:50:29,048] INFO: Iter 500 Summary: 
[2022-07-16 09:50:29,049] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07524581372737885
[2022-07-16 09:50:45,407] INFO: Iter 600 Summary: 
[2022-07-16 09:50:45,424] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07541789539158344
[2022-07-16 09:51:00,976] INFO: Iter 700 Summary: 
[2022-07-16 09:51:00,980] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07527600184082985
[2022-07-16 09:51:17,453] INFO: Iter 800 Summary: 
[2022-07-16 09:51:17,454] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07404811911284924
[2022-07-16 09:51:33,962] INFO: Iter 900 Summary: 
[2022-07-16 09:51:33,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06595692668110133
[2022-07-16 09:51:50,424] INFO: Iter 1000 Summary: 
[2022-07-16 09:51:50,442] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.060967629067599775
[2022-07-16 09:52:06,312] INFO: Iter 1100 Summary: 
[2022-07-16 09:52:06,329] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058663430772721764
[2022-07-16 09:52:23,329] INFO: Iter 1200 Summary: 
[2022-07-16 09:52:23,331] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05768703609704971
[2022-07-16 09:52:39,397] INFO: Iter 1300 Summary: 
[2022-07-16 09:52:39,399] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056604369543492794
[2022-07-16 09:52:55,787] INFO: Iter 1400 Summary: 
[2022-07-16 09:52:55,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05580500777810812
[2022-07-16 09:53:13,909] INFO: Iter 1500 Summary: 
[2022-07-16 09:53:13,927] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05492050539702177
[2022-07-16 09:53:29,315] INFO: Iter 1600 Summary: 
[2022-07-16 09:53:29,315] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053964214511215684
[2022-07-16 09:53:44,148] INFO: Iter 1700 Summary: 
[2022-07-16 09:53:44,149] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05409658443182707
[2022-07-16 09:54:00,985] INFO: Iter 1800 Summary: 
[2022-07-16 09:54:00,986] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05356252785772085
[2022-07-16 09:54:17,685] INFO: Iter 1900 Summary: 
[2022-07-16 09:54:17,686] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053710738010704515
[2022-07-16 09:54:34,794] INFO: Iter 2000 Summary: 
[2022-07-16 09:54:34,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053279403261840344
[2022-07-16 09:54:51,852] INFO: Iter 2100 Summary: 
[2022-07-16 09:54:51,870] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053017912469804286
[2022-07-16 09:55:09,617] INFO: Iter 2200 Summary: 
[2022-07-16 09:55:09,623] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05289290186017752
[2022-07-16 09:55:27,488] INFO: Iter 2300 Summary: 
[2022-07-16 09:55:27,493] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0529998841881752
[2022-07-16 09:55:45,213] INFO: Iter 2400 Summary: 
[2022-07-16 09:55:45,214] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05238912682980299
[2022-07-16 09:56:01,945] INFO: Iter 2500 Summary: 
[2022-07-16 09:56:01,962] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052170883528888226
[2022-07-16 09:56:19,111] INFO: Iter 2600 Summary: 
[2022-07-16 09:56:19,129] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05139263290911913
[2022-07-16 09:56:35,885] INFO: Iter 2700 Summary: 
[2022-07-16 09:56:35,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05130717266350984
[2022-07-16 09:56:52,589] INFO: Iter 2800 Summary: 
[2022-07-16 09:56:52,590] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05137701466679573
[2022-07-16 09:57:09,660] INFO: Iter 2900 Summary: 
[2022-07-16 09:57:09,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05081245061010122
[2022-07-16 09:57:27,204] INFO: Iter 3000 Summary: 
[2022-07-16 09:57:27,222] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05123129401355982
[2022-07-16 09:57:43,625] INFO: Iter 3100 Summary: 
[2022-07-16 09:57:43,625] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050762766264379026
[2022-07-16 09:58:00,457] INFO: Iter 3200 Summary: 
[2022-07-16 09:58:00,459] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050542991645634174
[2022-07-16 09:58:16,969] INFO: Iter 3300 Summary: 
[2022-07-16 09:58:16,971] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05047601036727428
[2022-07-16 09:58:33,350] INFO: Iter 3400 Summary: 
[2022-07-16 09:58:33,351] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05043482296168804
[2022-07-16 09:58:49,772] INFO: Iter 3500 Summary: 
[2022-07-16 09:58:49,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04987371575087309
[2022-07-16 09:59:07,333] INFO: Iter 3600 Summary: 
[2022-07-16 09:59:07,351] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050208612196147445
[2022-07-16 09:59:24,489] INFO: Iter 3700 Summary: 
[2022-07-16 09:59:24,506] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049786359257996085
[2022-07-16 09:59:41,013] INFO: Iter 3800 Summary: 
[2022-07-16 09:59:41,014] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05011823147535324
[2022-07-16 09:59:57,910] INFO: Iter 3900 Summary: 
[2022-07-16 09:59:57,912] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04997882828116417
[2022-07-16 10:00:14,778] INFO: Iter 4000 Summary: 
[2022-07-16 10:00:14,779] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04949166871607304
[2022-07-16 10:00:31,988] INFO: Iter 4100 Summary: 
[2022-07-16 10:00:32,006] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049745579175651075
[2022-07-16 10:00:48,710] INFO: Iter 4200 Summary: 
[2022-07-16 10:00:48,711] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04981742139905691
[2022-07-16 10:01:06,301] INFO: Iter 4300 Summary: 
[2022-07-16 10:01:06,302] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04998032260686159
[2022-07-16 10:01:23,637] INFO: Iter 4400 Summary: 
[2022-07-16 10:01:23,638] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04968548372387886
[2022-07-16 10:01:40,881] INFO: Iter 4500 Summary: 
[2022-07-16 10:01:40,898] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049304991587996484
[2022-07-16 10:01:57,764] INFO: Iter 4600 Summary: 
[2022-07-16 10:01:57,765] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04916872911155224
[2022-07-16 10:02:13,760] INFO: Iter 4700 Summary: 
[2022-07-16 10:02:13,778] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049462562203407286
[2022-07-16 10:02:30,647] INFO: Iter 4800 Summary: 
[2022-07-16 10:02:30,648] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04913660645484924
[2022-07-16 10:02:47,333] INFO: Iter 4900 Summary: 
[2022-07-16 10:02:47,334] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04941633131355047
[2022-07-16 10:03:05,326] INFO: Iter 5000 Summary: 
[2022-07-16 10:03:05,327] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04926725760102272
[2022-07-16 10:03:25,474] INFO: Iter 5100 Summary: 
[2022-07-16 10:03:25,477] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04900814518332482
[2022-07-16 10:03:41,877] INFO: Iter 5200 Summary: 
[2022-07-16 10:03:41,895] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0491867321357131
[2022-07-16 10:03:58,558] INFO: Iter 5300 Summary: 
[2022-07-16 10:03:58,576] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04923061195760965
[2022-07-16 10:04:15,235] INFO: Iter 5400 Summary: 
[2022-07-16 10:04:15,237] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04879134625196457
[2022-07-16 10:04:31,696] INFO: Iter 5500 Summary: 
[2022-07-16 10:04:31,696] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048292308412492276
[2022-07-16 10:04:48,413] INFO: Iter 5600 Summary: 
[2022-07-16 10:04:48,416] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048627188242971894
[2022-07-16 10:05:06,353] INFO: Iter 5700 Summary: 
[2022-07-16 10:05:06,372] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04880509670823813
[2022-07-16 10:05:23,021] INFO: Iter 5800 Summary: 
[2022-07-16 10:05:23,038] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04837856806814671
[2022-07-16 10:05:40,010] INFO: Iter 5900 Summary: 
[2022-07-16 10:05:40,011] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04868406143039465
[2022-07-16 10:05:57,413] INFO: Iter 6000 Summary: 
[2022-07-16 10:05:57,414] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04829354386776686
[2022-07-16 10:06:14,207] INFO: Iter 6100 Summary: 
[2022-07-16 10:06:14,208] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0482914612442255
[2022-07-16 10:06:31,276] INFO: Iter 6200 Summary: 
[2022-07-16 10:06:31,277] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048329561538994315
[2022-07-16 10:06:48,315] INFO: Iter 6300 Summary: 
[2022-07-16 10:06:48,333] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04844644006341696
[2022-07-16 10:07:05,178] INFO: Iter 6400 Summary: 
[2022-07-16 10:07:05,195] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04846217356622219
[2022-07-16 10:07:23,079] INFO: Iter 6500 Summary: 
[2022-07-16 10:07:23,080] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048289870657026765
[2022-07-16 10:07:40,586] INFO: Iter 6600 Summary: 
[2022-07-16 10:07:40,589] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048163787089288236
[2022-07-16 10:07:57,586] INFO: Iter 6700 Summary: 
[2022-07-16 10:07:57,588] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0478543321043253
[2022-07-16 10:08:14,946] INFO: Iter 6800 Summary: 
[2022-07-16 10:08:14,964] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048137200362980366
[2022-07-16 10:08:31,713] INFO: Iter 6900 Summary: 
[2022-07-16 10:08:31,714] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047723103649914264
[2022-07-16 10:08:47,964] INFO: Iter 7000 Summary: 
[2022-07-16 10:08:47,967] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047854083366692064
[2022-07-16 10:09:05,413] INFO: Iter 7100 Summary: 
[2022-07-16 10:09:05,416] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047783167734742164
[2022-07-16 10:09:23,461] INFO: Iter 7200 Summary: 
[2022-07-16 10:09:23,463] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04767759796231985
[2022-07-16 10:09:40,567] INFO: Iter 7300 Summary: 
[2022-07-16 10:09:40,568] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04760842200368643
[2022-07-16 10:09:58,156] INFO: Iter 7400 Summary: 
[2022-07-16 10:09:58,157] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047647067569196226
[2022-07-16 10:10:14,767] INFO: Iter 7500 Summary: 
[2022-07-16 10:10:14,768] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04832841657102108
[2022-07-16 10:10:32,486] INFO: Iter 7600 Summary: 
[2022-07-16 10:10:32,486] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047660996802151204
[2022-07-16 10:10:49,746] INFO: Iter 7700 Summary: 
[2022-07-16 10:10:49,747] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047414333745837214
[2022-07-16 10:11:06,404] INFO: Iter 7800 Summary: 
[2022-07-16 10:11:06,405] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04731194522231817
[2022-07-16 10:11:24,149] INFO: Iter 7900 Summary: 
[2022-07-16 10:11:24,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047289097383618355
[2022-07-16 10:11:42,165] INFO: Iter 8000 Summary: 
[2022-07-16 10:11:42,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04732777789235115
[2022-07-16 10:11:58,792] INFO: Iter 8100 Summary: 
[2022-07-16 10:11:58,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047344210110604766
[2022-07-16 10:12:15,794] INFO: Iter 8200 Summary: 
[2022-07-16 10:12:15,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047412636205554005
[2022-07-16 10:12:32,820] INFO: Iter 8300 Summary: 
[2022-07-16 10:12:32,821] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0472196651622653
[2022-07-16 10:12:49,864] INFO: Iter 8400 Summary: 
[2022-07-16 10:12:49,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047292203083634375
[2022-07-16 10:13:06,441] INFO: Iter 8500 Summary: 
[2022-07-16 10:13:06,442] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046900769546628
[2022-07-16 10:13:23,104] INFO: Iter 8600 Summary: 
[2022-07-16 10:13:23,121] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047575793415308
[2022-07-16 10:13:39,721] INFO: Iter 8700 Summary: 
[2022-07-16 10:13:39,722] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047079085856676105
[2022-07-16 10:13:55,851] INFO: Iter 8800 Summary: 
[2022-07-16 10:13:55,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04743941027671099
[2022-07-16 10:14:12,717] INFO: Iter 8900 Summary: 
[2022-07-16 10:14:12,718] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047289930917322635
[2022-07-16 10:14:28,992] INFO: Iter 9000 Summary: 
[2022-07-16 10:14:28,994] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046928950883448124
[2022-07-16 10:14:45,068] INFO: Iter 9100 Summary: 
[2022-07-16 10:14:45,085] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047143208906054494
[2022-07-16 10:15:00,809] INFO: Iter 9200 Summary: 
[2022-07-16 10:15:00,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04691059570759535
[2022-07-16 10:15:16,462] INFO: Iter 9300 Summary: 
[2022-07-16 10:15:16,463] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046913736164569855
[2022-07-16 10:15:34,767] INFO: Iter 9400 Summary: 
[2022-07-16 10:15:34,768] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04669858422130346
[2022-07-16 10:15:51,770] INFO: Iter 9500 Summary: 
[2022-07-16 10:15:51,787] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04675610985606909
[2022-07-16 10:16:09,326] INFO: Iter 9600 Summary: 
[2022-07-16 10:16:09,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046924650147557256
[2022-07-16 10:16:26,387] INFO: Iter 9700 Summary: 
[2022-07-16 10:16:26,387] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046771166138350966
[2022-07-16 10:16:42,747] INFO: Iter 9800 Summary: 
[2022-07-16 10:16:42,765] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04648545064032078
[2022-07-16 10:16:59,334] INFO: Iter 9900 Summary: 
[2022-07-16 10:16:59,352] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04682376530021429
[2022-07-16 10:17:17,848] INFO: Iter 10000 Summary: 
[2022-07-16 10:17:18,091] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046503708101809024
[2022-07-16 10:17:37,466] INFO: Iter 10100 Summary: 
[2022-07-16 10:17:37,499] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04648228906095028
[2022-07-16 10:17:55,819] INFO: Iter 10200 Summary: 
[2022-07-16 10:17:55,820] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04638626959174871
[2022-07-16 10:18:11,868] INFO: Iter 10300 Summary: 
[2022-07-16 10:18:11,868] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04707783449441195
[2022-07-16 10:18:27,932] INFO: Iter 10400 Summary: 
[2022-07-16 10:18:27,949] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04687384743243456
[2022-07-16 10:18:44,556] INFO: Iter 10500 Summary: 
[2022-07-16 10:18:44,557] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04641363777220249
[2022-07-16 10:19:01,124] INFO: Iter 10600 Summary: 
[2022-07-16 10:19:01,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046411081813275815
[2022-07-16 10:19:19,247] INFO: Iter 10700 Summary: 
[2022-07-16 10:19:19,248] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04638694997876883
[2022-07-16 10:19:35,641] INFO: Iter 10800 Summary: 
[2022-07-16 10:19:35,642] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04657036490738392
[2022-07-16 10:19:51,561] INFO: Iter 10900 Summary: 
[2022-07-16 10:19:51,562] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046656480319797994
[2022-07-16 10:20:08,086] INFO: Iter 11000 Summary: 
[2022-07-16 10:20:08,086] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046025007627904416
[2022-07-16 10:20:25,109] INFO: Iter 11100 Summary: 
[2022-07-16 10:20:25,126] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04598245866596699
[2022-07-16 10:20:42,236] INFO: Iter 11200 Summary: 
[2022-07-16 10:20:42,236] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046041418835520743
[2022-07-16 10:20:59,481] INFO: Iter 11300 Summary: 
[2022-07-16 10:20:59,483] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04631269995123148
[2022-07-16 10:21:17,044] INFO: Iter 11400 Summary: 
[2022-07-16 10:21:17,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04614110562950373
[2022-07-16 10:21:33,013] INFO: Iter 11500 Summary: 
[2022-07-16 10:21:33,016] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04618106946349144
[2022-07-16 10:21:49,533] INFO: Iter 11600 Summary: 
[2022-07-16 10:21:49,551] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04629002880305052
[2022-07-16 10:22:07,457] INFO: Iter 11700 Summary: 
[2022-07-16 10:22:07,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046020719371736046
[2022-07-16 10:22:23,701] INFO: Iter 11800 Summary: 
[2022-07-16 10:22:23,702] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046328343339264395
[2022-07-16 10:22:40,909] INFO: Iter 11900 Summary: 
[2022-07-16 10:22:40,911] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04605703260749579
[2022-07-16 10:22:57,472] INFO: Iter 12000 Summary: 
[2022-07-16 10:22:57,473] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04600391704589128
[2022-07-16 10:23:14,426] INFO: Iter 12100 Summary: 
[2022-07-16 10:23:14,443] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04591511055827141
[2022-07-16 10:23:31,047] INFO: Iter 12200 Summary: 
[2022-07-16 10:23:31,048] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04568201929330826
[2022-07-16 10:23:47,639] INFO: Iter 12300 Summary: 
[2022-07-16 10:23:47,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04601374056190252
[2022-07-16 10:24:04,449] INFO: Iter 12400 Summary: 
[2022-07-16 10:24:04,449] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04583740565925837
[2022-07-16 10:24:20,843] INFO: Iter 12500 Summary: 
[2022-07-16 10:24:20,846] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04610093206167221
[2022-07-16 10:24:37,641] INFO: Iter 12600 Summary: 
[2022-07-16 10:24:37,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04613475818186998
[2022-07-16 10:24:54,863] INFO: Iter 12700 Summary: 
[2022-07-16 10:24:54,880] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04590004526078701
[2022-07-16 10:25:11,430] INFO: Iter 12800 Summary: 
[2022-07-16 10:25:11,431] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04591037474572659
[2022-07-16 10:25:28,147] INFO: Iter 12900 Summary: 
[2022-07-16 10:25:28,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045770185329020024
[2022-07-16 10:25:44,914] INFO: Iter 13000 Summary: 
[2022-07-16 10:25:44,915] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045863139629364016
[2022-07-16 10:26:02,133] INFO: Iter 13100 Summary: 
[2022-07-16 10:26:02,134] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04582304507493973
[2022-07-16 10:26:19,373] INFO: Iter 13200 Summary: 
[2022-07-16 10:26:19,391] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04580389838665724
[2022-07-16 10:26:36,802] INFO: Iter 13300 Summary: 
[2022-07-16 10:26:36,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04564905345439911
[2022-07-16 10:26:53,921] INFO: Iter 13400 Summary: 
[2022-07-16 10:26:53,922] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04524258878082037
[2022-07-16 10:27:10,890] INFO: Iter 13500 Summary: 
[2022-07-16 10:27:10,891] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04549223154783249
[2022-07-16 10:27:28,414] INFO: Iter 13600 Summary: 
[2022-07-16 10:27:28,415] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04570742979645729
[2022-07-16 10:27:45,143] INFO: Iter 13700 Summary: 
[2022-07-16 10:27:45,160] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045721323750913145
[2022-07-16 10:28:01,332] INFO: Iter 13800 Summary: 
[2022-07-16 10:28:01,337] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045622094608843325
[2022-07-16 10:28:18,757] INFO: Iter 13900 Summary: 
[2022-07-16 10:28:18,775] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045616213902831076
[2022-07-16 10:28:35,886] INFO: Iter 14000 Summary: 
[2022-07-16 10:28:35,888] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04539230920374394
[2022-07-16 10:28:52,577] INFO: Iter 14100 Summary: 
[2022-07-16 10:28:52,579] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04550016023218632
[2022-07-16 10:29:09,803] INFO: Iter 14200 Summary: 
[2022-07-16 10:29:09,804] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04532018281519413
[2022-07-16 10:29:25,932] INFO: Iter 14300 Summary: 
[2022-07-16 10:29:25,934] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04550552986562252
[2022-07-16 10:29:42,272] INFO: Iter 14400 Summary: 
[2022-07-16 10:29:42,272] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04540949679911137
[2022-07-16 10:29:59,142] INFO: Iter 14500 Summary: 
[2022-07-16 10:29:59,144] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04557432636618614
[2022-07-16 10:30:15,635] INFO: Iter 14600 Summary: 
[2022-07-16 10:30:15,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04527395185083151
[2022-07-16 10:30:33,377] INFO: Iter 14700 Summary: 
[2022-07-16 10:30:33,379] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045472602024674416
[2022-07-16 10:30:50,176] INFO: Iter 14800 Summary: 
[2022-07-16 10:30:50,177] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04556714046746493
[2022-07-16 10:31:07,229] INFO: Iter 14900 Summary: 
[2022-07-16 10:31:07,245] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045340009555220606
[2022-07-16 10:31:24,995] INFO: Iter 15000 Summary: 
[2022-07-16 10:31:24,996] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045363726392388344
[2022-07-16 10:31:44,763] INFO: Iter 15100 Summary: 
[2022-07-16 10:31:44,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04547381240874529
[2022-07-16 10:32:01,604] INFO: Iter 15200 Summary: 
[2022-07-16 10:32:01,605] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04524617303162813
[2022-07-16 10:32:18,618] INFO: Iter 15300 Summary: 
[2022-07-16 10:32:18,618] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04512097753584385
[2022-07-16 10:32:35,714] INFO: Iter 15400 Summary: 
[2022-07-16 10:32:35,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04545921377837658
[2022-07-16 10:32:52,816] INFO: Iter 15500 Summary: 
[2022-07-16 10:32:52,833] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0452788070961833
[2022-07-16 10:33:09,494] INFO: Iter 15600 Summary: 
[2022-07-16 10:33:09,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04539645064622164
[2022-07-16 10:33:27,039] INFO: Iter 15700 Summary: 
[2022-07-16 10:33:27,040] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04520460866391659
[2022-07-16 10:33:44,584] INFO: Iter 15800 Summary: 
[2022-07-16 10:33:44,585] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04544510025531054
[2022-07-16 10:34:01,781] INFO: Iter 15900 Summary: 
[2022-07-16 10:34:01,784] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045049668587744236
[2022-07-16 10:34:18,363] INFO: Iter 16000 Summary: 
[2022-07-16 10:34:18,363] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04498372111469507
[2022-07-16 10:34:34,873] INFO: Iter 16100 Summary: 
[2022-07-16 10:34:34,890] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04538688875734806
[2022-07-16 10:34:52,049] INFO: Iter 16200 Summary: 
[2022-07-16 10:34:52,065] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04503101408481598
[2022-07-16 10:35:08,665] INFO: Iter 16300 Summary: 
[2022-07-16 10:35:08,677] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045150124654173854
[2022-07-16 10:35:25,943] INFO: Iter 16400 Summary: 
[2022-07-16 10:35:25,960] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044950374141335486
[2022-07-16 10:35:42,847] INFO: Iter 16500 Summary: 
[2022-07-16 10:35:42,849] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04536027409136295
[2022-07-16 10:35:59,776] INFO: Iter 16600 Summary: 
[2022-07-16 10:35:59,793] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04484634395688772
[2022-07-16 10:36:16,800] INFO: Iter 16700 Summary: 
[2022-07-16 10:36:16,818] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04513044014573097
[2022-07-16 10:36:33,651] INFO: Iter 16800 Summary: 
[2022-07-16 10:36:33,652] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04500014059245586
[2022-07-16 10:36:50,342] INFO: Iter 16900 Summary: 
[2022-07-16 10:36:50,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04504231918603182
[2022-07-16 10:37:07,570] INFO: Iter 17000 Summary: 
[2022-07-16 10:37:07,587] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04505197759717703
[2022-07-16 10:37:25,510] INFO: Iter 17100 Summary: 
[2022-07-16 10:37:25,528] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045053835920989514
[2022-07-16 10:37:42,744] INFO: Iter 17200 Summary: 
[2022-07-16 10:37:42,762] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04521119352430105
[2022-07-16 10:37:59,936] INFO: Iter 17300 Summary: 
[2022-07-16 10:37:59,941] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04476533330976963
[2022-07-16 10:38:16,771] INFO: Iter 17400 Summary: 
[2022-07-16 10:38:16,773] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04504880126565695
[2022-07-16 10:38:33,881] INFO: Iter 17500 Summary: 
[2022-07-16 10:38:33,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04447690561413765
[2022-07-16 10:38:51,150] INFO: Iter 17600 Summary: 
[2022-07-16 10:38:51,168] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045190799571573737
[2022-07-16 10:39:08,487] INFO: Iter 17700 Summary: 
[2022-07-16 10:39:08,504] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04475557558238506
[2022-07-16 10:39:25,630] INFO: Iter 17800 Summary: 
[2022-07-16 10:39:25,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04497839216142893
[2022-07-16 10:39:42,465] INFO: Iter 17900 Summary: 
[2022-07-16 10:39:42,466] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04496672034263611
[2022-07-16 10:39:59,460] INFO: Iter 18000 Summary: 
[2022-07-16 10:39:59,462] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04509263593703508
[2022-07-16 10:40:15,429] INFO: Iter 18100 Summary: 
[2022-07-16 10:40:15,446] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04479413125663996
[2022-07-16 10:40:32,978] INFO: Iter 18200 Summary: 
[2022-07-16 10:40:32,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0447000652179122
[2022-07-16 10:40:49,876] INFO: Iter 18300 Summary: 
[2022-07-16 10:40:49,877] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0448617484793067
[2022-07-16 10:41:06,869] INFO: Iter 18400 Summary: 
[2022-07-16 10:41:06,870] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04489338468760252
[2022-07-16 10:41:23,758] INFO: Iter 18500 Summary: 
[2022-07-16 10:41:23,759] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04477775026112795
[2022-07-16 10:41:40,669] INFO: Iter 18600 Summary: 
[2022-07-16 10:41:40,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04443577256053686
[2022-07-16 10:41:57,949] INFO: Iter 18700 Summary: 
[2022-07-16 10:41:57,950] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045031567737460136
[2022-07-16 10:42:14,960] INFO: Iter 18800 Summary: 
[2022-07-16 10:42:14,980] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04448860794305801
[2022-07-16 10:42:31,798] INFO: Iter 18900 Summary: 
[2022-07-16 10:42:31,799] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04530449617654085
[2022-07-16 10:42:49,046] INFO: Iter 19000 Summary: 
[2022-07-16 10:42:49,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04474093496799469
[2022-07-16 10:43:06,091] INFO: Iter 19100 Summary: 
[2022-07-16 10:43:06,092] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04476071581244469
[2022-07-16 10:43:22,644] INFO: Iter 19200 Summary: 
[2022-07-16 10:43:22,644] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044744106382131575
[2022-07-16 10:43:40,922] INFO: Iter 19300 Summary: 
[2022-07-16 10:43:40,939] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044508654735982416
[2022-07-16 10:43:57,212] INFO: Iter 19400 Summary: 
[2022-07-16 10:43:57,238] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04454664468765259
[2022-07-16 10:44:14,087] INFO: Iter 19500 Summary: 
[2022-07-16 10:44:14,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044738058485090736
[2022-07-16 10:44:31,284] INFO: Iter 19600 Summary: 
[2022-07-16 10:44:31,285] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04467423789203167
[2022-07-16 10:44:48,268] INFO: Iter 19700 Summary: 
[2022-07-16 10:44:48,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04471179056912661
[2022-07-16 10:45:05,346] INFO: Iter 19800 Summary: 
[2022-07-16 10:45:05,364] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044370823837816716
[2022-07-16 10:45:22,537] INFO: Iter 19900 Summary: 
[2022-07-16 10:45:22,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04465390499681234
[2022-07-16 10:45:40,192] INFO: Iter 20000 Summary: 
[2022-07-16 10:45:40,194] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044518215246498585
[2022-07-16 10:46:00,228] INFO: Iter 20100 Summary: 
[2022-07-16 10:46:00,229] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04478961393237114
[2022-07-16 10:46:17,672] INFO: Iter 20200 Summary: 
[2022-07-16 10:46:17,674] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04483071260154247
[2022-07-16 10:46:34,607] INFO: Iter 20300 Summary: 
[2022-07-16 10:46:34,607] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449937865138054
[2022-07-16 10:46:51,642] INFO: Iter 20400 Summary: 
[2022-07-16 10:46:51,660] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459170863032341
[2022-07-16 10:47:07,973] INFO: Iter 20500 Summary: 
[2022-07-16 10:47:07,974] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434978496283293
[2022-07-16 10:47:25,609] INFO: Iter 20600 Summary: 
[2022-07-16 10:47:25,627] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449662808328867
[2022-07-16 10:47:43,203] INFO: Iter 20700 Summary: 
[2022-07-16 10:47:43,216] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044774251766502855
[2022-07-16 10:48:00,494] INFO: Iter 20800 Summary: 
[2022-07-16 10:48:00,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04484433498233557
[2022-07-16 10:48:17,479] INFO: Iter 20900 Summary: 
[2022-07-16 10:48:17,480] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04457191035151482
[2022-07-16 10:48:34,670] INFO: Iter 21000 Summary: 
[2022-07-16 10:48:34,670] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04462894294410944
[2022-07-16 10:48:51,746] INFO: Iter 21100 Summary: 
[2022-07-16 10:48:51,763] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044489840418100356
[2022-07-16 10:49:08,824] INFO: Iter 21200 Summary: 
[2022-07-16 10:49:08,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436304457485676
[2022-07-16 10:49:26,086] INFO: Iter 21300 Summary: 
[2022-07-16 10:49:26,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044168811775743964
[2022-07-16 10:49:43,809] INFO: Iter 21400 Summary: 
[2022-07-16 10:49:43,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044602636843919755
[2022-07-16 10:50:00,740] INFO: Iter 21500 Summary: 
[2022-07-16 10:50:00,757] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044477133117616176
[2022-07-16 10:50:18,222] INFO: Iter 21600 Summary: 
[2022-07-16 10:50:18,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04415593959391117
[2022-07-16 10:50:35,561] INFO: Iter 21700 Summary: 
[2022-07-16 10:50:35,561] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043966590762138366
[2022-07-16 10:50:53,249] INFO: Iter 21800 Summary: 
[2022-07-16 10:50:53,250] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04480412941426039
[2022-07-16 10:51:09,932] INFO: Iter 21900 Summary: 
[2022-07-16 10:51:09,934] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044707836471498015
[2022-07-16 10:51:26,288] INFO: Iter 22000 Summary: 
[2022-07-16 10:51:26,289] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044028421454131605
[2022-07-16 10:51:43,760] INFO: Iter 22100 Summary: 
[2022-07-16 10:51:43,777] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044526108466088773
[2022-07-16 10:52:00,729] INFO: Iter 22200 Summary: 
[2022-07-16 10:52:00,754] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04437551628798246
[2022-07-16 10:52:17,118] INFO: Iter 22300 Summary: 
[2022-07-16 10:52:17,132] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04442794427275658
[2022-07-16 10:52:34,029] INFO: Iter 22400 Summary: 
[2022-07-16 10:52:34,029] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04445121757686138
[2022-07-16 10:52:50,667] INFO: Iter 22500 Summary: 
[2022-07-16 10:52:50,667] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444696069136262
[2022-07-16 10:53:08,314] INFO: Iter 22600 Summary: 
[2022-07-16 10:53:08,333] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418407004326582
[2022-07-16 10:53:25,427] INFO: Iter 22700 Summary: 
[2022-07-16 10:53:25,444] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04441584218293428
[2022-07-16 10:53:43,175] INFO: Iter 22800 Summary: 
[2022-07-16 10:53:43,175] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044368752427399155
[2022-07-16 10:54:00,472] INFO: Iter 22900 Summary: 
[2022-07-16 10:54:00,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044475561268627645
[2022-07-16 10:54:16,965] INFO: Iter 23000 Summary: 
[2022-07-16 10:54:16,966] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04429416365921497
[2022-07-16 10:54:33,902] INFO: Iter 23100 Summary: 
[2022-07-16 10:54:33,919] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0441903043538332
[2022-07-16 10:54:50,739] INFO: Iter 23200 Summary: 
[2022-07-16 10:54:50,743] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04456519726663828
[2022-07-16 10:55:05,899] INFO: Iter 23300 Summary: 
[2022-07-16 10:55:05,900] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04460661455988884
[2022-07-16 10:55:23,346] INFO: Iter 23400 Summary: 
[2022-07-16 10:55:23,353] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398983262479305
[2022-07-16 10:55:40,625] INFO: Iter 23500 Summary: 
[2022-07-16 10:55:40,626] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04416174188256264
[2022-07-16 10:55:57,439] INFO: Iter 23600 Summary: 
[2022-07-16 10:55:57,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044273005612194535
[2022-07-16 10:56:13,727] INFO: Iter 23700 Summary: 
[2022-07-16 10:56:13,745] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04438310138881207
[2022-07-16 10:56:31,604] INFO: Iter 23800 Summary: 
[2022-07-16 10:56:31,621] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044330619685351846
[2022-07-16 10:56:48,732] INFO: Iter 23900 Summary: 
[2022-07-16 10:56:48,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04424164857715368
[2022-07-16 10:57:06,230] INFO: Iter 24000 Summary: 
[2022-07-16 10:57:06,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04440360512584448
[2022-07-16 10:57:23,701] INFO: Iter 24100 Summary: 
[2022-07-16 10:57:23,702] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04443230304867029
[2022-07-16 10:57:40,627] INFO: Iter 24200 Summary: 
[2022-07-16 10:57:40,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04410341139882803
[2022-07-16 10:57:57,877] INFO: Iter 24300 Summary: 
[2022-07-16 10:57:57,895] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382157225161791
[2022-07-16 10:58:14,598] INFO: Iter 24400 Summary: 
[2022-07-16 10:58:14,598] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444693136960268
[2022-07-16 10:58:31,366] INFO: Iter 24500 Summary: 
[2022-07-16 10:58:31,367] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0446012619137764
[2022-07-16 10:58:48,190] INFO: Iter 24600 Summary: 
[2022-07-16 10:58:48,190] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044455689825117585
[2022-07-16 10:59:05,266] INFO: Iter 24700 Summary: 
[2022-07-16 10:59:05,267] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044172475226223466
[2022-07-16 10:59:22,936] INFO: Iter 24800 Summary: 
[2022-07-16 10:59:22,954] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043838885612785816
[2022-07-16 10:59:39,338] INFO: Iter 24900 Summary: 
[2022-07-16 10:59:39,369] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044250159412622454
[2022-07-16 10:59:56,471] INFO: Iter 25000 Summary: 
[2022-07-16 10:59:56,472] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044111883416771885
[2022-07-16 11:00:17,255] INFO: Iter 25100 Summary: 
[2022-07-16 11:00:17,258] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044253023155033586
[2022-07-16 11:00:34,568] INFO: Iter 25200 Summary: 
[2022-07-16 11:00:34,568] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04414278898388147
[2022-07-16 11:00:51,302] INFO: Iter 25300 Summary: 
[2022-07-16 11:00:51,302] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398620322346687
[2022-07-16 11:01:08,507] INFO: Iter 25400 Summary: 
[2022-07-16 11:01:08,525] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04433657713234425
[2022-07-16 11:01:25,161] INFO: Iter 25500 Summary: 
[2022-07-16 11:01:25,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04412601299583912
[2022-07-16 11:01:42,402] INFO: Iter 25600 Summary: 
[2022-07-16 11:01:42,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404476765543222
[2022-07-16 11:01:59,764] INFO: Iter 25700 Summary: 
[2022-07-16 11:01:59,767] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04417889680713415
[2022-07-16 11:02:16,837] INFO: Iter 25800 Summary: 
[2022-07-16 11:02:16,840] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043795100413262844
[2022-07-16 11:02:33,436] INFO: Iter 25900 Summary: 
[2022-07-16 11:02:33,437] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044116004295647146
[2022-07-16 11:02:50,174] INFO: Iter 26000 Summary: 
[2022-07-16 11:02:50,191] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399545352905989
[2022-07-16 11:03:07,340] INFO: Iter 26100 Summary: 
[2022-07-16 11:03:07,358] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399340607225895
[2022-07-16 11:03:24,468] INFO: Iter 26200 Summary: 
[2022-07-16 11:03:24,469] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044011506251990795
[2022-07-16 11:03:41,573] INFO: Iter 26300 Summary: 
[2022-07-16 11:03:41,574] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044643985964357855
[2022-07-16 11:03:59,359] INFO: Iter 26400 Summary: 
[2022-07-16 11:03:59,360] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044047739915549756
[2022-07-16 11:04:15,999] INFO: Iter 26500 Summary: 
[2022-07-16 11:04:16,016] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044073847308754924
[2022-07-16 11:04:33,043] INFO: Iter 26600 Summary: 
[2022-07-16 11:04:33,043] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043825442492961886
[2022-07-16 11:04:49,314] INFO: Iter 26700 Summary: 
[2022-07-16 11:04:49,317] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385838896036148
[2022-07-16 11:05:06,417] INFO: Iter 26800 Summary: 
[2022-07-16 11:05:06,418] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04412813924252987
[2022-07-16 11:05:23,400] INFO: Iter 26900 Summary: 
[2022-07-16 11:05:23,431] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04414485029876232
[2022-07-16 11:05:40,569] INFO: Iter 27000 Summary: 
[2022-07-16 11:05:40,596] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404898829758167
[2022-07-16 11:05:58,511] INFO: Iter 27100 Summary: 
[2022-07-16 11:05:58,548] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044120081253349784
[2022-07-16 11:06:15,253] INFO: Iter 27200 Summary: 
[2022-07-16 11:06:15,254] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04428716760128736
[2022-07-16 11:06:32,514] INFO: Iter 27300 Summary: 
[2022-07-16 11:06:32,515] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04360299538820982
[2022-07-16 11:06:49,801] INFO: Iter 27400 Summary: 
[2022-07-16 11:06:49,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04439182512462139
[2022-07-16 11:07:06,664] INFO: Iter 27500 Summary: 
[2022-07-16 11:07:06,681] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043702203407883645
[2022-07-16 11:07:23,613] INFO: Iter 27600 Summary: 
[2022-07-16 11:07:23,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043968421891331676
[2022-07-16 11:07:40,461] INFO: Iter 27700 Summary: 
[2022-07-16 11:07:40,462] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04408939406275749
[2022-07-16 11:07:58,377] INFO: Iter 27800 Summary: 
[2022-07-16 11:07:58,377] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04390971720218659
[2022-07-16 11:08:14,694] INFO: Iter 27900 Summary: 
[2022-07-16 11:08:14,698] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043683912120759484
[2022-07-16 11:08:31,229] INFO: Iter 28000 Summary: 
[2022-07-16 11:08:31,230] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04409276593476534
[2022-07-16 11:08:48,477] INFO: Iter 28100 Summary: 
[2022-07-16 11:08:48,495] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404610402882099
[2022-07-16 11:09:05,831] INFO: Iter 28200 Summary: 
[2022-07-16 11:09:05,849] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04393255766481161
[2022-07-16 11:09:23,347] INFO: Iter 28300 Summary: 
[2022-07-16 11:09:23,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04410839483141899
[2022-07-16 11:09:40,891] INFO: Iter 28400 Summary: 
[2022-07-16 11:09:40,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04393178001046181
[2022-07-16 11:09:58,592] INFO: Iter 28500 Summary: 
[2022-07-16 11:09:58,593] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044304105192422866
[2022-07-16 11:10:14,916] INFO: Iter 28600 Summary: 
[2022-07-16 11:10:14,920] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04390644494444132
[2022-07-16 11:10:31,954] INFO: Iter 28700 Summary: 
[2022-07-16 11:10:31,954] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0440754983946681
[2022-07-16 11:10:48,550] INFO: Iter 28800 Summary: 
[2022-07-16 11:10:48,552] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04402519375085831
[2022-07-16 11:11:05,185] INFO: Iter 28900 Summary: 
[2022-07-16 11:11:05,192] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0435989623144269
[2022-07-16 11:11:22,151] INFO: Iter 29000 Summary: 
[2022-07-16 11:11:22,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04376814037561417
[2022-07-16 11:11:39,134] INFO: Iter 29100 Summary: 
[2022-07-16 11:11:39,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04403993934392929
[2022-07-16 11:11:56,643] INFO: Iter 29200 Summary: 
[2022-07-16 11:11:56,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04414166316390038
[2022-07-16 11:12:13,601] INFO: Iter 29300 Summary: 
[2022-07-16 11:12:13,602] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043568234443664554
[2022-07-16 11:12:30,542] INFO: Iter 29400 Summary: 
[2022-07-16 11:12:30,545] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0443497946485877
[2022-07-16 11:12:47,439] INFO: Iter 29500 Summary: 
[2022-07-16 11:12:47,440] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04402396596968174
[2022-07-16 11:13:04,099] INFO: Iter 29600 Summary: 
[2022-07-16 11:13:04,117] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04386769261211157
[2022-07-16 11:13:21,298] INFO: Iter 29700 Summary: 
[2022-07-16 11:13:21,311] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04415319129824638
[2022-07-16 11:13:37,940] INFO: Iter 29800 Summary: 
[2022-07-16 11:13:37,941] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380744352936745
[2022-07-16 11:13:55,162] INFO: Iter 29900 Summary: 
[2022-07-16 11:13:55,163] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043698310740292075
[2022-07-16 11:14:12,642] INFO: Iter 30000 Summary: 
[2022-07-16 11:14:12,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043765790686011316
[2022-07-16 11:14:33,007] INFO: Iter 30100 Summary: 
[2022-07-16 11:14:33,027] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.04350139312446118
[2022-07-16 11:14:49,799] INFO: Iter 30200 Summary: 
[2022-07-16 11:14:49,800] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04347876321524382
[2022-07-16 11:15:07,003] INFO: Iter 30300 Summary: 
[2022-07-16 11:15:07,005] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043525151908397675
[2022-07-16 11:15:23,790] INFO: Iter 30400 Summary: 
[2022-07-16 11:15:23,790] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04299985837191343
[2022-07-16 11:15:41,564] INFO: Iter 30500 Summary: 
[2022-07-16 11:15:41,565] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04316225439310074
[2022-07-16 11:15:58,909] INFO: Iter 30600 Summary: 
[2022-07-16 11:15:58,927] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04309284329414367
[2022-07-16 11:16:16,533] INFO: Iter 30700 Summary: 
[2022-07-16 11:16:16,547] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043226394653320316
[2022-07-16 11:16:33,506] INFO: Iter 30800 Summary: 
[2022-07-16 11:16:33,507] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04329367004334927
[2022-07-16 11:16:50,323] INFO: Iter 30900 Summary: 
[2022-07-16 11:16:50,324] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04313574019819498
[2022-07-16 11:17:06,426] INFO: Iter 31000 Summary: 
[2022-07-16 11:17:06,429] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043040216453373435
[2022-07-16 11:17:22,578] INFO: Iter 31100 Summary: 
[2022-07-16 11:17:22,578] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043064769953489304
[2022-07-16 11:17:37,760] INFO: Iter 31200 Summary: 
[2022-07-16 11:17:37,761] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04327495615929365
[2022-07-16 11:17:53,685] INFO: Iter 31300 Summary: 
[2022-07-16 11:17:53,685] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04315076969563961
[2022-07-16 11:18:10,460] INFO: Iter 31400 Summary: 
[2022-07-16 11:18:10,478] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042904207184910774
[2022-07-16 11:18:26,422] INFO: Iter 31500 Summary: 
[2022-07-16 11:18:26,424] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043225622214376924
[2022-07-16 11:18:42,838] INFO: Iter 31600 Summary: 
[2022-07-16 11:18:42,838] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042992875277996064
[2022-07-16 11:18:59,087] INFO: Iter 31700 Summary: 
[2022-07-16 11:18:59,088] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043345598243176936
[2022-07-16 11:19:16,034] INFO: Iter 31800 Summary: 
[2022-07-16 11:19:16,044] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043192258030176164
[2022-07-16 11:19:33,060] INFO: Iter 31900 Summary: 
[2022-07-16 11:19:33,060] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042907656244933604
[2022-07-16 11:19:50,033] INFO: Iter 32000 Summary: 
[2022-07-16 11:19:50,051] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0429919258132577
[2022-07-16 11:20:07,416] INFO: Iter 32100 Summary: 
[2022-07-16 11:20:07,417] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0431859852001071
[2022-07-16 11:20:23,570] INFO: Iter 32200 Summary: 
[2022-07-16 11:20:23,571] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04309896036982536
[2022-07-16 11:20:40,439] INFO: Iter 32300 Summary: 
[2022-07-16 11:20:40,441] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043158443309366704
[2022-07-16 11:20:57,220] INFO: Iter 32400 Summary: 
[2022-07-16 11:20:57,236] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04297866538167
[2022-07-16 11:21:14,056] INFO: Iter 32500 Summary: 
[2022-07-16 11:21:14,074] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04304405838251114
[2022-07-16 11:21:30,791] INFO: Iter 32600 Summary: 
[2022-07-16 11:21:30,792] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04318237207829952
[2022-07-16 11:21:47,536] INFO: Iter 32700 Summary: 
[2022-07-16 11:21:47,537] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0429201677441597
[2022-07-16 11:22:05,728] INFO: Iter 32800 Summary: 
[2022-07-16 11:22:05,734] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04305995639413595
[2022-07-16 11:22:22,877] INFO: Iter 32900 Summary: 
[2022-07-16 11:22:22,877] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0429685727879405
[2022-07-16 11:22:40,028] INFO: Iter 33000 Summary: 
[2022-07-16 11:22:40,045] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04296182747930288
[2022-07-16 11:22:56,881] INFO: Iter 33100 Summary: 
[2022-07-16 11:22:56,882] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043425537496805194
[2022-07-16 11:23:13,245] INFO: Iter 33200 Summary: 
[2022-07-16 11:23:13,249] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04307710226625204
[2022-07-16 11:23:30,172] INFO: Iter 33300 Summary: 
[2022-07-16 11:23:30,172] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043197701647877694
[2022-07-16 11:23:47,097] INFO: Iter 33400 Summary: 
[2022-07-16 11:23:47,099] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042793298028409484
[2022-07-16 11:24:04,255] INFO: Iter 33500 Summary: 
[2022-07-16 11:24:04,285] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04331863317638636
[2022-07-16 11:24:20,995] INFO: Iter 33600 Summary: 
[2022-07-16 11:24:20,997] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042933877594769
[2022-07-16 11:24:38,060] INFO: Iter 33700 Summary: 
[2022-07-16 11:24:38,062] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04333079006522894
[2022-07-16 11:24:55,019] INFO: Iter 33800 Summary: 
[2022-07-16 11:24:55,020] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04289436731487513
[2022-07-16 11:25:11,980] INFO: Iter 33900 Summary: 
[2022-07-16 11:25:11,983] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04297188811004162
[2022-07-16 11:25:28,881] INFO: Iter 34000 Summary: 
[2022-07-16 11:25:28,883] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04310289237648249
[2022-07-16 11:25:45,394] INFO: Iter 34100 Summary: 
[2022-07-16 11:25:45,395] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04304200567305088
[2022-07-16 11:26:02,781] INFO: Iter 34200 Summary: 
[2022-07-16 11:26:02,783] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04312695022672415
[2022-07-16 11:26:19,233] INFO: Iter 34300 Summary: 
[2022-07-16 11:26:19,234] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043012609519064424
[2022-07-16 11:26:36,246] INFO: Iter 34400 Summary: 
[2022-07-16 11:26:36,247] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04327204421162605
[2022-07-16 11:26:53,117] INFO: Iter 34500 Summary: 
[2022-07-16 11:26:53,118] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04299019865691662
[2022-07-16 11:27:09,824] INFO: Iter 34600 Summary: 
[2022-07-16 11:27:09,825] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043155947774648665
[2022-07-16 11:27:26,918] INFO: Iter 34700 Summary: 
[2022-07-16 11:27:26,919] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04314815126359463
[2022-07-16 11:27:44,300] INFO: Iter 34800 Summary: 
[2022-07-16 11:27:44,300] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04305017028003931
[2022-07-16 11:28:01,248] INFO: Iter 34900 Summary: 
[2022-07-16 11:28:01,250] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04302193865180016
[2022-07-16 11:28:18,206] INFO: Iter 35000 Summary: 
[2022-07-16 11:28:18,223] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04282990273088217
[2022-07-16 11:28:37,171] INFO: Iter 35100 Summary: 
[2022-07-16 11:28:37,172] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04297451004385948
[2022-07-16 11:28:53,152] INFO: Iter 35200 Summary: 
[2022-07-16 11:28:53,154] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04319514699280262
[2022-07-16 11:29:10,285] INFO: Iter 35300 Summary: 
[2022-07-16 11:29:10,285] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04322635769844055
[2022-07-16 11:29:26,807] INFO: Iter 35400 Summary: 
[2022-07-16 11:29:26,808] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04280364915728569
[2022-07-16 11:29:43,443] INFO: Iter 35500 Summary: 
[2022-07-16 11:29:43,444] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043125421851873395
[2022-07-16 11:30:00,979] INFO: Iter 35600 Summary: 
[2022-07-16 11:30:00,996] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042975125573575496
[2022-07-16 11:30:18,630] INFO: Iter 35700 Summary: 
[2022-07-16 11:30:18,649] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04323242474347353
[2022-07-16 11:30:35,300] INFO: Iter 35800 Summary: 
[2022-07-16 11:30:35,301] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04298187244683504
[2022-07-16 11:30:52,570] INFO: Iter 35900 Summary: 
[2022-07-16 11:30:52,588] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04288449600338936
[2022-07-16 11:31:09,992] INFO: Iter 36000 Summary: 
[2022-07-16 11:31:09,993] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04306629240512848
[2022-07-16 11:31:27,302] INFO: Iter 36100 Summary: 
[2022-07-16 11:31:27,304] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0431176533550024
[2022-07-16 11:31:44,520] INFO: Iter 36200 Summary: 
[2022-07-16 11:31:44,538] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04323066100478172
[2022-07-16 11:32:01,773] INFO: Iter 36300 Summary: 
[2022-07-16 11:32:01,777] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04284242048859596
[2022-07-16 11:32:19,389] INFO: Iter 36400 Summary: 
[2022-07-16 11:32:19,398] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043148493729531764
[2022-07-16 11:32:35,719] INFO: Iter 36500 Summary: 
[2022-07-16 11:32:35,737] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043110514841973785
[2022-07-16 11:32:52,967] INFO: Iter 36600 Summary: 
[2022-07-16 11:32:52,968] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04341776810586453
[2022-07-16 11:33:09,359] INFO: Iter 36700 Summary: 
[2022-07-16 11:33:09,360] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04313388086855412
[2022-07-16 11:33:26,001] INFO: Iter 36800 Summary: 
[2022-07-16 11:33:26,003] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04283587619662285
[2022-07-16 11:33:43,010] INFO: Iter 36900 Summary: 
[2022-07-16 11:33:43,011] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04309696450829506
[2022-07-16 11:33:58,955] INFO: Iter 37000 Summary: 
[2022-07-16 11:33:58,958] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0428772958740592
[2022-07-16 11:34:16,405] INFO: Iter 37100 Summary: 
[2022-07-16 11:34:16,406] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04302628237754107
[2022-07-16 11:34:33,931] INFO: Iter 37200 Summary: 
[2022-07-16 11:34:33,949] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04324377223849297
[2022-07-16 11:34:51,378] INFO: Iter 37300 Summary: 
[2022-07-16 11:34:51,379] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04296496879309416
[2022-07-16 11:35:08,519] INFO: Iter 37400 Summary: 
[2022-07-16 11:35:08,521] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04298974171280861
[2022-07-16 11:35:24,396] INFO: Iter 37500 Summary: 
[2022-07-16 11:35:24,397] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042692908085882664
[2022-07-16 11:35:41,157] INFO: Iter 37600 Summary: 
[2022-07-16 11:35:41,175] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043197035975754264
[2022-07-16 11:35:58,375] INFO: Iter 37700 Summary: 
[2022-07-16 11:35:58,378] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04303626142442227
[2022-07-16 11:36:15,625] INFO: Iter 37800 Summary: 
[2022-07-16 11:36:15,627] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043037087544798854
[2022-07-16 11:36:32,728] INFO: Iter 37900 Summary: 
[2022-07-16 11:36:32,730] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043182987906038764
[2022-07-16 11:36:49,667] INFO: Iter 38000 Summary: 
[2022-07-16 11:36:49,668] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04317398171871901
[2022-07-16 11:37:05,971] INFO: Iter 38100 Summary: 
[2022-07-16 11:37:05,975] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04319994270801544
[2022-07-16 11:37:22,937] INFO: Iter 38200 Summary: 
[2022-07-16 11:37:22,938] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042713468819856645
[2022-07-16 11:37:40,378] INFO: Iter 38300 Summary: 
[2022-07-16 11:37:40,380] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043164288997650145
[2022-07-16 11:37:56,762] INFO: Iter 38400 Summary: 
[2022-07-16 11:37:56,763] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04288391381502152
[2022-07-16 11:38:14,085] INFO: Iter 38500 Summary: 
[2022-07-16 11:38:14,086] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043089424967765806
[2022-07-16 11:38:31,569] INFO: Iter 38600 Summary: 
[2022-07-16 11:38:31,571] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04300607897341251
[2022-07-16 11:38:48,650] INFO: Iter 38700 Summary: 
[2022-07-16 11:38:48,651] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042924050241708755
[2022-07-16 11:39:05,721] INFO: Iter 38800 Summary: 
[2022-07-16 11:39:05,722] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043524312973022464
[2022-07-16 11:39:22,868] INFO: Iter 38900 Summary: 
[2022-07-16 11:39:22,887] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04271838739514351
[2022-07-16 11:39:40,027] INFO: Iter 39000 Summary: 
[2022-07-16 11:39:40,028] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04328507732599974
[2022-07-16 11:39:56,969] INFO: Iter 39100 Summary: 
[2022-07-16 11:39:56,973] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042908116318285464
[2022-07-16 11:40:14,203] INFO: Iter 39200 Summary: 
[2022-07-16 11:40:14,220] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04295456025749445
[2022-07-16 11:40:31,231] INFO: Iter 39300 Summary: 
[2022-07-16 11:40:31,233] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043293489180505275
[2022-07-16 11:40:47,978] INFO: Iter 39400 Summary: 
[2022-07-16 11:40:47,981] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04279654789716005
[2022-07-16 11:41:04,935] INFO: Iter 39500 Summary: 
[2022-07-16 11:41:04,936] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.042886487320065496
[2022-07-16 11:41:21,564] INFO: Iter 39600 Summary: 
[2022-07-16 11:41:21,565] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04308646865189075
[2022-07-16 11:41:38,398] INFO: Iter 39700 Summary: 
[2022-07-16 11:41:38,400] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043082703575491904
[2022-07-16 11:41:55,659] INFO: Iter 39800 Summary: 
[2022-07-16 11:41:55,659] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043129133097827434
[2022-07-16 11:42:13,621] INFO: Iter 39900 Summary: 
[2022-07-16 11:42:13,622] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043167132139205935
[2022-07-16 11:42:30,733] INFO: Iter 40000 Summary: 
[2022-07-16 11:42:30,735] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.043318558558821676
