[2022-07-14 22:40:44,672] INFO: {
    "abs_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m",
    "batch_size": 256,
    "cos_lr_max": 1e-05,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 40000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "h36m_anno_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/data/h36m/",
    "link_log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/log_last.log",
    "link_val_log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/val_last.log",
    "log_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log",
    "log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/log_2022_07_14_22_38_51.log",
    "model_pth": null,
    "motion": {
        "dim": 66,
        "h36m_input_length": 50,
        "h36m_input_length_dct": 50,
        "h36m_target_length": 10,
        "h36m_target_length_eval": 25,
        "h36m_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": false,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 66,
        "init_w_trunc_normal": true,
        "out_features": 66,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 66,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "repo_name": "siMLPe_my",
    "root_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my",
    "save_every": 5000,
    "seed": 304,
    "shift_step": 1,
    "snapshot_dir": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/snapshot",
    "this_dir": "baseline_h36m",
    "use_relative_loss": true,
    "val_log_file": "/home/jianwei/yanjiu/siMLPe/siMLPe_my/exps/baseline_h36m/log/val_2022_07_14_22_38_51.log",
    "weight_decay": 0.0001
}
[2022-07-14 22:41:21,569] INFO: Iter 100 Summary: 
[2022-07-14 22:41:21,577] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.10683845736086368
[2022-07-14 22:41:31,278] INFO: Iter 200 Summary: 
[2022-07-14 22:41:31,279] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0751369509845972
[2022-07-14 22:41:40,465] INFO: Iter 300 Summary: 
[2022-07-14 22:41:40,465] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07431463561952115
[2022-07-14 22:41:49,746] INFO: Iter 400 Summary: 
[2022-07-14 22:41:49,746] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0704981316626072
[2022-07-14 22:42:07,807] INFO: Iter 500 Summary: 
[2022-07-14 22:42:07,807] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06551273375749588
[2022-07-14 22:42:17,032] INFO: Iter 600 Summary: 
[2022-07-14 22:42:17,033] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.060997788049280646
[2022-07-14 22:42:26,489] INFO: Iter 700 Summary: 
[2022-07-14 22:42:26,489] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05893341992050409
[2022-07-14 22:42:35,172] INFO: Iter 800 Summary: 
[2022-07-14 22:42:35,172] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0573938775062561
[2022-07-14 22:42:44,302] INFO: Iter 900 Summary: 
[2022-07-14 22:42:44,302] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05641561288386583
[2022-07-14 22:42:53,381] INFO: Iter 1000 Summary: 
[2022-07-14 22:42:53,381] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05565407864749432
[2022-07-14 22:43:13,957] INFO: Iter 1100 Summary: 
[2022-07-14 22:43:13,958] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05502686180174351
[2022-07-14 22:43:23,394] INFO: Iter 1200 Summary: 
[2022-07-14 22:43:23,395] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054770777188241485
[2022-07-14 22:43:33,073] INFO: Iter 1300 Summary: 
[2022-07-14 22:43:33,074] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054265413731336594
[2022-07-14 22:43:42,544] INFO: Iter 1400 Summary: 
[2022-07-14 22:43:42,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054026846587657926
[2022-07-14 22:43:51,743] INFO: Iter 1500 Summary: 
[2022-07-14 22:43:51,743] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05324739977717399
[2022-07-14 22:44:01,045] INFO: Iter 1600 Summary: 
[2022-07-14 22:44:01,045] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05318814851343632
[2022-07-14 22:44:19,515] INFO: Iter 1700 Summary: 
[2022-07-14 22:44:19,515] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053136532865464686
[2022-07-14 22:44:28,718] INFO: Iter 1800 Summary: 
[2022-07-14 22:44:28,718] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052301891185343265
[2022-07-14 22:44:37,712] INFO: Iter 1900 Summary: 
[2022-07-14 22:44:37,712] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052581470385193826
[2022-07-14 22:44:46,911] INFO: Iter 2000 Summary: 
[2022-07-14 22:44:46,911] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05202224627137184
[2022-07-14 22:44:56,574] INFO: Iter 2100 Summary: 
[2022-07-14 22:44:56,575] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05165532801300287
[2022-07-14 22:45:17,209] INFO: Iter 2200 Summary: 
[2022-07-14 22:45:17,209] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05151415389031172
[2022-07-14 22:45:26,764] INFO: Iter 2300 Summary: 
[2022-07-14 22:45:26,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05136409301310778
[2022-07-14 22:45:35,988] INFO: Iter 2400 Summary: 
[2022-07-14 22:45:35,989] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050731693990528584
[2022-07-14 22:45:45,587] INFO: Iter 2500 Summary: 
[2022-07-14 22:45:45,587] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05085084095597267
[2022-07-14 22:45:54,708] INFO: Iter 2600 Summary: 
[2022-07-14 22:45:54,708] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050118405781686304
[2022-07-14 22:46:03,390] INFO: Iter 2700 Summary: 
[2022-07-14 22:46:03,391] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05087905000895262
[2022-07-14 22:46:21,095] INFO: Iter 2800 Summary: 
[2022-07-14 22:46:21,096] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05028617970645428
[2022-07-14 22:46:30,316] INFO: Iter 2900 Summary: 
[2022-07-14 22:46:30,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0501128501817584
[2022-07-14 22:46:39,601] INFO: Iter 3000 Summary: 
[2022-07-14 22:46:39,601] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05028868470340967
[2022-07-14 22:46:48,851] INFO: Iter 3100 Summary: 
[2022-07-14 22:46:48,851] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049772908203303816
[2022-07-14 22:46:58,056] INFO: Iter 3200 Summary: 
[2022-07-14 22:46:58,056] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04963100101798773
[2022-07-14 22:47:07,366] INFO: Iter 3300 Summary: 
[2022-07-14 22:47:07,367] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04911849666386843
[2022-07-14 22:47:25,065] INFO: Iter 3400 Summary: 
[2022-07-14 22:47:25,065] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049457439370453354
[2022-07-14 22:47:34,757] INFO: Iter 3500 Summary: 
[2022-07-14 22:47:34,758] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049075603932142255
[2022-07-14 22:47:43,954] INFO: Iter 3600 Summary: 
[2022-07-14 22:47:43,954] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04893553752452135
[2022-07-14 22:47:53,498] INFO: Iter 3700 Summary: 
[2022-07-14 22:47:53,499] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049232916347682475
[2022-07-14 22:48:02,977] INFO: Iter 3800 Summary: 
[2022-07-14 22:48:02,977] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04907449994236231
[2022-07-14 22:48:20,865] INFO: Iter 3900 Summary: 
[2022-07-14 22:48:20,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04889932241290808
[2022-07-14 22:48:30,163] INFO: Iter 4000 Summary: 
[2022-07-14 22:48:30,163] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04821832474321127
[2022-07-14 22:48:39,591] INFO: Iter 4100 Summary: 
[2022-07-14 22:48:39,591] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048153414987027646
[2022-07-14 22:48:49,248] INFO: Iter 4200 Summary: 
[2022-07-14 22:48:49,248] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04873027112334967
[2022-07-14 22:48:58,601] INFO: Iter 4300 Summary: 
[2022-07-14 22:48:58,601] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048527175709605214
[2022-07-14 22:49:08,194] INFO: Iter 4400 Summary: 
[2022-07-14 22:49:08,195] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0479836069047451
[2022-07-14 22:49:26,440] INFO: Iter 4500 Summary: 
[2022-07-14 22:49:26,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04829808883368969
[2022-07-14 22:49:35,827] INFO: Iter 4600 Summary: 
[2022-07-14 22:49:35,827] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047972631193697456
[2022-07-14 22:49:45,065] INFO: Iter 4700 Summary: 
[2022-07-14 22:49:45,065] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048141726702451704
[2022-07-14 22:49:53,624] INFO: Iter 4800 Summary: 
[2022-07-14 22:49:53,624] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0478582402318716
[2022-07-14 22:50:03,216] INFO: Iter 4900 Summary: 
[2022-07-14 22:50:03,216] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04795054573565721
[2022-07-14 22:50:11,743] INFO: Iter 5000 Summary: 
[2022-07-14 22:50:11,744] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04789582159370184
[2022-07-14 22:50:34,094] INFO: Iter 5100 Summary: 
[2022-07-14 22:50:34,094] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04763835605233908
[2022-07-14 22:50:43,244] INFO: Iter 5200 Summary: 
[2022-07-14 22:50:43,245] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04777873169630766
[2022-07-14 22:50:52,691] INFO: Iter 5300 Summary: 
[2022-07-14 22:50:52,691] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04708854380995035
[2022-07-14 22:51:01,281] INFO: Iter 5400 Summary: 
[2022-07-14 22:51:01,281] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04734821889549494
[2022-07-14 22:51:10,718] INFO: Iter 5500 Summary: 
[2022-07-14 22:51:10,718] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04732767798006535
[2022-07-14 22:51:32,108] INFO: Iter 5600 Summary: 
[2022-07-14 22:51:32,108] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04751702141016722
[2022-07-14 22:51:41,864] INFO: Iter 5700 Summary: 
[2022-07-14 22:51:41,864] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04748359832912683
[2022-07-14 22:51:51,478] INFO: Iter 5800 Summary: 
[2022-07-14 22:51:51,479] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047070236168801786
[2022-07-14 22:52:00,874] INFO: Iter 5900 Summary: 
[2022-07-14 22:52:00,875] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046619262918829917
[2022-07-14 22:52:10,207] INFO: Iter 6000 Summary: 
[2022-07-14 22:52:10,207] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04650726482272148
[2022-07-14 22:52:19,343] INFO: Iter 6100 Summary: 
[2022-07-14 22:52:19,344] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04672047536820173
[2022-07-14 22:52:41,337] INFO: Iter 6200 Summary: 
[2022-07-14 22:52:41,337] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04636389445513487
[2022-07-14 22:52:50,613] INFO: Iter 6300 Summary: 
[2022-07-14 22:52:50,613] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04640252366662025
[2022-07-14 22:52:59,706] INFO: Iter 6400 Summary: 
[2022-07-14 22:52:59,707] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04642920345067978
[2022-07-14 22:53:09,236] INFO: Iter 6500 Summary: 
[2022-07-14 22:53:09,236] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04633110962808132
[2022-07-14 22:53:18,624] INFO: Iter 6600 Summary: 
[2022-07-14 22:53:18,625] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045965436995029446
[2022-07-14 22:53:40,184] INFO: Iter 6700 Summary: 
[2022-07-14 22:53:40,185] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046640123128890994
[2022-07-14 22:53:49,606] INFO: Iter 6800 Summary: 
[2022-07-14 22:53:49,606] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04618043020367622
[2022-07-14 22:53:59,151] INFO: Iter 6900 Summary: 
[2022-07-14 22:53:59,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046572883874177934
[2022-07-14 22:54:08,230] INFO: Iter 7000 Summary: 
[2022-07-14 22:54:08,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04601952210068703
[2022-07-14 22:54:17,048] INFO: Iter 7100 Summary: 
[2022-07-14 22:54:17,049] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0460965571179986
[2022-07-14 22:54:36,787] INFO: Iter 7200 Summary: 
[2022-07-14 22:54:36,787] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04579590007662773
[2022-07-14 22:54:45,926] INFO: Iter 7300 Summary: 
[2022-07-14 22:54:45,926] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045799643136560916
[2022-07-14 22:54:55,078] INFO: Iter 7400 Summary: 
[2022-07-14 22:54:55,078] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0457764707505703
[2022-07-14 22:55:04,400] INFO: Iter 7500 Summary: 
[2022-07-14 22:55:04,401] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04560261655598879
[2022-07-14 22:55:14,059] INFO: Iter 7600 Summary: 
[2022-07-14 22:55:14,060] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04537761092185974
[2022-07-14 22:55:23,301] INFO: Iter 7700 Summary: 
[2022-07-14 22:55:23,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04545761428773403
[2022-07-14 22:55:42,275] INFO: Iter 7800 Summary: 
[2022-07-14 22:55:42,276] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04532938230782747
[2022-07-14 22:55:51,877] INFO: Iter 7900 Summary: 
[2022-07-14 22:55:51,878] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04552041292190552
[2022-07-14 22:56:00,982] INFO: Iter 8000 Summary: 
[2022-07-14 22:56:00,983] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045308120474219324
[2022-07-14 22:56:10,164] INFO: Iter 8100 Summary: 
[2022-07-14 22:56:10,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04558177594095469
[2022-07-14 22:56:19,955] INFO: Iter 8200 Summary: 
[2022-07-14 22:56:19,956] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04510863371193409
[2022-07-14 22:56:40,796] INFO: Iter 8300 Summary: 
[2022-07-14 22:56:40,797] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04484763346612453
[2022-07-14 22:56:50,123] INFO: Iter 8400 Summary: 
[2022-07-14 22:56:50,123] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04504496518522501
[2022-07-14 22:56:59,415] INFO: Iter 8500 Summary: 
[2022-07-14 22:56:59,415] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461100861430168
[2022-07-14 22:57:08,686] INFO: Iter 8600 Summary: 
[2022-07-14 22:57:08,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04492440942674875
[2022-07-14 22:57:17,804] INFO: Iter 8700 Summary: 
[2022-07-14 22:57:17,804] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04478022865951061
[2022-07-14 22:57:26,789] INFO: Iter 8800 Summary: 
[2022-07-14 22:57:26,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04463778972625732
[2022-07-14 22:57:43,720] INFO: Iter 8900 Summary: 
[2022-07-14 22:57:43,720] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04485488671809435
[2022-07-14 22:57:53,720] INFO: Iter 9000 Summary: 
[2022-07-14 22:57:53,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04465014733374119
[2022-07-14 22:58:02,945] INFO: Iter 9100 Summary: 
[2022-07-14 22:58:02,945] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04474134415388107
[2022-07-14 22:58:12,237] INFO: Iter 9200 Summary: 
[2022-07-14 22:58:12,237] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434468332678079
[2022-07-14 22:58:21,323] INFO: Iter 9300 Summary: 
[2022-07-14 22:58:21,324] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044589626006782054
[2022-07-14 22:58:30,661] INFO: Iter 9400 Summary: 
[2022-07-14 22:58:30,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04403625387698412
[2022-07-14 22:58:48,446] INFO: Iter 9500 Summary: 
[2022-07-14 22:58:48,446] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044186174497008325
[2022-07-14 22:58:58,552] INFO: Iter 9600 Summary: 
[2022-07-14 22:58:58,553] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04374502208083868
[2022-07-14 22:59:08,751] INFO: Iter 9700 Summary: 
[2022-07-14 22:59:08,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044324089363217356
[2022-07-14 22:59:17,845] INFO: Iter 9800 Summary: 
[2022-07-14 22:59:17,845] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436577308923006
[2022-07-14 22:59:27,191] INFO: Iter 9900 Summary: 
[2022-07-14 22:59:27,192] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404796577990055
[2022-07-14 22:59:47,656] INFO: Iter 10000 Summary: 
[2022-07-14 22:59:47,656] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04392045479267836
[2022-07-14 22:59:59,320] INFO: Iter 10100 Summary: 
[2022-07-14 22:59:59,320] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043955284543335436
[2022-07-14 23:00:10,279] INFO: Iter 10200 Summary: 
[2022-07-14 23:00:10,279] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04397307001054287
[2022-07-14 23:00:21,409] INFO: Iter 10300 Summary: 
[2022-07-14 23:00:21,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04383315250277519
[2022-07-14 23:00:32,151] INFO: Iter 10400 Summary: 
[2022-07-14 23:00:32,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366184052079916
[2022-07-14 23:00:52,528] INFO: Iter 10500 Summary: 
[2022-07-14 23:00:52,528] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404822655022144
[2022-07-14 23:01:01,644] INFO: Iter 10600 Summary: 
[2022-07-14 23:01:01,644] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04365687489509582
[2022-07-14 23:01:10,840] INFO: Iter 10700 Summary: 
[2022-07-14 23:01:10,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043296904265880586
[2022-07-14 23:01:20,138] INFO: Iter 10800 Summary: 
[2022-07-14 23:01:20,138] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355186607688665
[2022-07-14 23:01:29,218] INFO: Iter 10900 Summary: 
[2022-07-14 23:01:29,219] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329119745641947
[2022-07-14 23:01:49,248] INFO: Iter 11000 Summary: 
[2022-07-14 23:01:49,249] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359216891229153
[2022-07-14 23:01:58,896] INFO: Iter 11100 Summary: 
[2022-07-14 23:01:58,896] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04279918033629656
[2022-07-14 23:02:08,008] INFO: Iter 11200 Summary: 
[2022-07-14 23:02:08,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04367800299078226
[2022-07-14 23:02:17,036] INFO: Iter 11300 Summary: 
[2022-07-14 23:02:17,036] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043522214256227015
[2022-07-14 23:02:26,315] INFO: Iter 11400 Summary: 
[2022-07-14 23:02:26,315] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04352513678371906
[2022-07-14 23:02:35,873] INFO: Iter 11500 Summary: 
[2022-07-14 23:02:35,873] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043379980288445946
[2022-07-14 23:02:55,169] INFO: Iter 11600 Summary: 
[2022-07-14 23:02:55,169] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04350618727505207
[2022-07-14 23:03:04,935] INFO: Iter 11700 Summary: 
[2022-07-14 23:03:04,935] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042976518124341966
[2022-07-14 23:03:14,431] INFO: Iter 11800 Summary: 
[2022-07-14 23:03:14,431] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330570481717586
[2022-07-14 23:03:24,383] INFO: Iter 11900 Summary: 
[2022-07-14 23:03:24,383] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04302939791232348
[2022-07-14 23:03:34,002] INFO: Iter 12000 Summary: 
[2022-07-14 23:03:34,003] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04295370411127806
[2022-07-14 23:03:43,609] INFO: Iter 12100 Summary: 
[2022-07-14 23:03:43,609] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042855474166572094
[2022-07-14 23:03:59,506] INFO: Iter 12200 Summary: 
[2022-07-14 23:03:59,506] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04303527060896158
[2022-07-14 23:04:09,046] INFO: Iter 12300 Summary: 
[2022-07-14 23:04:09,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04300246428698301
[2022-07-14 23:04:18,477] INFO: Iter 12400 Summary: 
[2022-07-14 23:04:18,478] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042778722904622556
[2022-07-14 23:04:27,202] INFO: Iter 12500 Summary: 
[2022-07-14 23:04:27,203] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04291264835745096
[2022-07-14 23:04:35,774] INFO: Iter 12600 Summary: 
[2022-07-14 23:04:35,775] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042819361127913
[2022-07-14 23:04:44,881] INFO: Iter 12700 Summary: 
[2022-07-14 23:04:44,881] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04271587930619716
[2022-07-14 23:05:06,259] INFO: Iter 12800 Summary: 
[2022-07-14 23:05:06,260] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04283963080495596
[2022-07-14 23:05:15,187] INFO: Iter 12900 Summary: 
[2022-07-14 23:05:15,187] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042690738216042516
[2022-07-14 23:05:24,671] INFO: Iter 13000 Summary: 
[2022-07-14 23:05:24,672] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042652452848851684
[2022-07-14 23:05:34,562] INFO: Iter 13100 Summary: 
[2022-07-14 23:05:34,563] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042530108392238614
[2022-07-14 23:05:43,559] INFO: Iter 13200 Summary: 
[2022-07-14 23:05:43,559] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0425553660467267
[2022-07-14 23:06:03,393] INFO: Iter 13300 Summary: 
[2022-07-14 23:06:03,394] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0421931067854166
[2022-07-14 23:06:12,626] INFO: Iter 13400 Summary: 
[2022-07-14 23:06:12,626] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04261534560471773
[2022-07-14 23:06:22,178] INFO: Iter 13500 Summary: 
[2022-07-14 23:06:22,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04294211883097887
[2022-07-14 23:06:31,715] INFO: Iter 13600 Summary: 
[2022-07-14 23:06:31,716] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042671686112880705
[2022-07-14 23:06:41,130] INFO: Iter 13700 Summary: 
[2022-07-14 23:06:41,131] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04249420888721943
[2022-07-14 23:06:50,779] INFO: Iter 13800 Summary: 
[2022-07-14 23:06:50,780] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04260342001914978
[2022-07-14 23:07:08,160] INFO: Iter 13900 Summary: 
[2022-07-14 23:07:08,161] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042494210936129095
[2022-07-14 23:07:17,757] INFO: Iter 14000 Summary: 
[2022-07-14 23:07:17,757] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04246777679771185
[2022-07-14 23:07:27,412] INFO: Iter 14100 Summary: 
[2022-07-14 23:07:27,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042258979454636574
[2022-07-14 23:07:36,451] INFO: Iter 14200 Summary: 
[2022-07-14 23:07:36,452] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04257101558148861
[2022-07-14 23:07:45,626] INFO: Iter 14300 Summary: 
[2022-07-14 23:07:45,627] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04228538013994694
[2022-07-14 23:07:55,333] INFO: Iter 14400 Summary: 
[2022-07-14 23:07:55,333] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042269874662160874
[2022-07-14 23:08:14,687] INFO: Iter 14500 Summary: 
[2022-07-14 23:08:14,688] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042144585326313974
[2022-07-14 23:08:23,289] INFO: Iter 14600 Summary: 
[2022-07-14 23:08:23,290] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04243559140712023
[2022-07-14 23:08:32,159] INFO: Iter 14700 Summary: 
[2022-07-14 23:08:32,160] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04204048909246921
[2022-07-14 23:08:41,227] INFO: Iter 14800 Summary: 
[2022-07-14 23:08:41,228] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04238765861839056
[2022-07-14 23:08:50,217] INFO: Iter 14900 Summary: 
[2022-07-14 23:08:50,218] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04210627250373364
[2022-07-14 23:08:59,530] INFO: Iter 15000 Summary: 
[2022-07-14 23:08:59,531] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04227913178503513
[2022-07-14 23:09:24,140] INFO: Iter 15100 Summary: 
[2022-07-14 23:09:24,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04244926985353231
[2022-07-14 23:09:33,786] INFO: Iter 15200 Summary: 
[2022-07-14 23:09:33,787] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04201218336820602
[2022-07-14 23:09:42,895] INFO: Iter 15300 Summary: 
[2022-07-14 23:09:42,896] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04168424360454082
[2022-07-14 23:09:52,344] INFO: Iter 15400 Summary: 
[2022-07-14 23:09:52,345] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04200475454330444
[2022-07-14 23:10:01,124] INFO: Iter 15500 Summary: 
[2022-07-14 23:10:01,125] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042052694000303745
[2022-07-14 23:10:22,239] INFO: Iter 15600 Summary: 
[2022-07-14 23:10:22,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04189146712422371
[2022-07-14 23:10:31,746] INFO: Iter 15700 Summary: 
[2022-07-14 23:10:31,747] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042018526159226896
[2022-07-14 23:10:40,847] INFO: Iter 15800 Summary: 
[2022-07-14 23:10:40,847] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0420285002887249
[2022-07-14 23:10:51,065] INFO: Iter 15900 Summary: 
[2022-07-14 23:10:51,066] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04215460155159235
[2022-07-14 23:11:00,425] INFO: Iter 16000 Summary: 
[2022-07-14 23:11:00,425] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042065574377775194
[2022-07-14 23:11:22,456] INFO: Iter 16100 Summary: 
[2022-07-14 23:11:22,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04180006951093674
[2022-07-14 23:11:31,892] INFO: Iter 16200 Summary: 
[2022-07-14 23:11:31,893] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04195431377738714
[2022-07-14 23:11:41,132] INFO: Iter 16300 Summary: 
[2022-07-14 23:11:41,132] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04189068451523781
[2022-07-14 23:11:49,966] INFO: Iter 16400 Summary: 
[2022-07-14 23:11:49,966] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04197051998227835
[2022-07-14 23:11:59,109] INFO: Iter 16500 Summary: 
[2022-07-14 23:11:59,109] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04180585328489542
[2022-07-14 23:12:08,592] INFO: Iter 16600 Summary: 
[2022-07-14 23:12:08,593] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04197142664343119
[2022-07-14 23:13:32,820] INFO: Iter 16700 Summary: 
[2022-07-14 23:13:32,830] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04188875511288643
[2022-07-14 23:13:41,865] INFO: Iter 16800 Summary: 
[2022-07-14 23:13:41,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04149137247353792
[2022-07-14 23:13:51,084] INFO: Iter 16900 Summary: 
[2022-07-14 23:13:51,084] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0415799368172884
[2022-07-14 23:14:00,162] INFO: Iter 17000 Summary: 
[2022-07-14 23:14:00,163] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04181912384927273
[2022-07-14 23:14:09,280] INFO: Iter 17100 Summary: 
[2022-07-14 23:14:09,281] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04189751554280519
[2022-07-14 23:14:27,166] INFO: Iter 17200 Summary: 
[2022-07-14 23:14:27,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04164799615740776
[2022-07-14 23:14:36,902] INFO: Iter 17300 Summary: 
[2022-07-14 23:14:36,903] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041467446237802505
[2022-07-14 23:14:46,158] INFO: Iter 17400 Summary: 
[2022-07-14 23:14:46,158] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04142213355749846
[2022-07-14 23:14:55,188] INFO: Iter 17500 Summary: 
[2022-07-14 23:14:55,188] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04148074936121702
[2022-07-14 23:15:04,439] INFO: Iter 17600 Summary: 
[2022-07-14 23:15:04,439] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04194463260471821
[2022-07-14 23:15:13,803] INFO: Iter 17700 Summary: 
[2022-07-14 23:15:13,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04184236142784357
[2022-07-14 23:15:31,699] INFO: Iter 17800 Summary: 
[2022-07-14 23:15:31,699] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04176571808755398
[2022-07-14 23:15:41,607] INFO: Iter 17900 Summary: 
[2022-07-14 23:15:41,607] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04172081220895052
[2022-07-14 23:15:51,002] INFO: Iter 18000 Summary: 
[2022-07-14 23:15:51,002] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04186526037752628
[2022-07-14 23:16:00,179] INFO: Iter 18100 Summary: 
[2022-07-14 23:16:00,180] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04158885855227709
[2022-07-14 23:16:09,534] INFO: Iter 18200 Summary: 
[2022-07-14 23:16:09,534] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04151668187230825
[2022-07-14 23:16:29,279] INFO: Iter 18300 Summary: 
[2022-07-14 23:16:29,280] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04139490399509668
[2022-07-14 23:16:39,983] INFO: Iter 18400 Summary: 
[2022-07-14 23:16:39,983] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041568267457187176
[2022-07-14 23:16:51,173] INFO: Iter 18500 Summary: 
[2022-07-14 23:16:51,173] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04170714549720287
[2022-07-14 23:17:01,270] INFO: Iter 18600 Summary: 
[2022-07-14 23:17:01,270] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041346833743155
[2022-07-14 23:17:10,560] INFO: Iter 18700 Summary: 
[2022-07-14 23:17:10,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04162906557321548
[2022-07-14 23:17:32,206] INFO: Iter 18800 Summary: 
[2022-07-14 23:17:32,206] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041336466409266
[2022-07-14 23:17:41,195] INFO: Iter 18900 Summary: 
[2022-07-14 23:17:41,195] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0415541822463274
[2022-07-14 23:17:50,927] INFO: Iter 19000 Summary: 
[2022-07-14 23:17:50,927] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04118372526019812
[2022-07-14 23:18:00,885] INFO: Iter 19100 Summary: 
[2022-07-14 23:18:00,885] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04115927718579769
[2022-07-14 23:18:09,884] INFO: Iter 19200 Summary: 
[2022-07-14 23:18:09,885] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041417594775557516
[2022-07-14 23:18:20,689] INFO: Iter 19300 Summary: 
[2022-07-14 23:18:20,690] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041457393541932105
[2022-07-14 23:18:42,280] INFO: Iter 19400 Summary: 
[2022-07-14 23:18:42,280] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04138319216668606
[2022-07-14 23:18:52,173] INFO: Iter 19500 Summary: 
[2022-07-14 23:18:52,174] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04149313379079103
[2022-07-14 23:19:01,282] INFO: Iter 19600 Summary: 
[2022-07-14 23:19:01,282] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04142596054822206
[2022-07-14 23:19:10,839] INFO: Iter 19700 Summary: 
[2022-07-14 23:19:10,840] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04145772069692612
[2022-07-14 23:19:20,112] INFO: Iter 19800 Summary: 
[2022-07-14 23:19:20,113] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041427751444280146
[2022-07-14 23:19:40,026] INFO: Iter 19900 Summary: 
[2022-07-14 23:19:40,027] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04134414806962013
[2022-07-14 23:19:50,204] INFO: Iter 20000 Summary: 
[2022-07-14 23:19:50,204] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04114116284996271
[2022-07-14 23:20:01,271] INFO: Iter 20100 Summary: 
[2022-07-14 23:20:01,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041240733675658704
[2022-07-14 23:20:10,300] INFO: Iter 20200 Summary: 
[2022-07-14 23:20:10,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04104646429419517
[2022-07-14 23:20:18,964] INFO: Iter 20300 Summary: 
[2022-07-14 23:20:18,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041397855281829835
[2022-07-14 23:20:36,621] INFO: Iter 20400 Summary: 
[2022-07-14 23:20:36,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041227242723107335
[2022-07-14 23:20:45,457] INFO: Iter 20500 Summary: 
[2022-07-14 23:20:45,457] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04159574333578348
[2022-07-14 23:20:54,330] INFO: Iter 20600 Summary: 
[2022-07-14 23:20:54,330] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04144307926297188
[2022-07-14 23:21:03,752] INFO: Iter 20700 Summary: 
[2022-07-14 23:21:03,753] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041249997764825824
[2022-07-14 23:21:12,898] INFO: Iter 20800 Summary: 
[2022-07-14 23:21:12,898] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041306716948747636
[2022-07-14 23:21:21,939] INFO: Iter 20900 Summary: 
[2022-07-14 23:21:21,940] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04132005844265223
[2022-07-14 23:21:37,000] INFO: Iter 21000 Summary: 
[2022-07-14 23:21:37,000] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041487298421561715
[2022-07-14 23:21:46,025] INFO: Iter 21100 Summary: 
[2022-07-14 23:21:46,025] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04155260227620602
[2022-07-14 23:21:54,873] INFO: Iter 21200 Summary: 
[2022-07-14 23:21:54,873] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041177737414836886
[2022-07-14 23:22:03,910] INFO: Iter 21300 Summary: 
[2022-07-14 23:22:03,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041077110767364505
[2022-07-14 23:22:13,702] INFO: Iter 21400 Summary: 
[2022-07-14 23:22:13,703] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04095494497567415
[2022-07-14 23:22:22,732] INFO: Iter 21500 Summary: 
[2022-07-14 23:22:22,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04102684874087572
[2022-07-14 23:23:53,216] INFO: Iter 21600 Summary: 
[2022-07-14 23:23:53,216] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04112509045749903
[2022-07-14 23:24:02,328] INFO: Iter 21700 Summary: 
[2022-07-14 23:24:02,328] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041180411837995055
[2022-07-14 23:24:11,629] INFO: Iter 21800 Summary: 
[2022-07-14 23:24:11,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04129165068268776
[2022-07-14 23:24:20,705] INFO: Iter 21900 Summary: 
[2022-07-14 23:24:20,706] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041333629861474035
[2022-07-14 23:24:29,874] INFO: Iter 22000 Summary: 
[2022-07-14 23:24:29,874] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041138864569365975
[2022-07-14 23:24:46,741] INFO: Iter 22100 Summary: 
[2022-07-14 23:24:46,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04113141529262066
[2022-07-14 23:24:56,057] INFO: Iter 22200 Summary: 
[2022-07-14 23:24:56,058] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04112049289047718
[2022-07-14 23:25:05,759] INFO: Iter 22300 Summary: 
[2022-07-14 23:25:05,760] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04097217306494713
[2022-07-14 23:25:15,574] INFO: Iter 22400 Summary: 
[2022-07-14 23:25:15,575] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04109909038990736
[2022-07-14 23:25:24,786] INFO: Iter 22500 Summary: 
[2022-07-14 23:25:24,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04136468045413494
[2022-07-14 23:25:34,093] INFO: Iter 22600 Summary: 
[2022-07-14 23:25:34,093] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04092433881014586
[2022-07-14 23:25:49,162] INFO: Iter 22700 Summary: 
[2022-07-14 23:25:49,163] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04093264162540436
[2022-07-14 23:25:59,160] INFO: Iter 22800 Summary: 
[2022-07-14 23:25:59,160] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04071120753884316
[2022-07-14 23:26:08,407] INFO: Iter 22900 Summary: 
[2022-07-14 23:26:08,407] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04088996034115553
[2022-07-14 23:26:17,858] INFO: Iter 23000 Summary: 
[2022-07-14 23:26:17,858] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04068007111549377
[2022-07-14 23:26:27,118] INFO: Iter 23100 Summary: 
[2022-07-14 23:26:27,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04102296713739634
[2022-07-14 23:26:36,432] INFO: Iter 23200 Summary: 
[2022-07-14 23:26:36,432] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040947887375950814
[2022-07-14 23:26:58,916] INFO: Iter 23300 Summary: 
[2022-07-14 23:26:58,916] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04136141337454319
[2022-07-14 23:27:08,461] INFO: Iter 23400 Summary: 
[2022-07-14 23:27:08,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04117933373898268
[2022-07-14 23:27:17,769] INFO: Iter 23500 Summary: 
[2022-07-14 23:27:17,770] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04119019400328398
[2022-07-14 23:27:26,982] INFO: Iter 23600 Summary: 
[2022-07-14 23:27:26,982] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04099202163517475
[2022-07-14 23:27:36,198] INFO: Iter 23700 Summary: 
[2022-07-14 23:27:36,198] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04077945664525032
[2022-07-14 23:27:45,288] INFO: Iter 23800 Summary: 
[2022-07-14 23:27:45,289] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040807822048664094
[2022-07-14 23:27:53,750] INFO: Iter 23900 Summary: 
[2022-07-14 23:27:53,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04103094272315502
[2022-07-14 23:28:10,017] INFO: Iter 24000 Summary: 
[2022-07-14 23:28:10,017] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040939573980867866
[2022-07-14 23:28:19,924] INFO: Iter 24100 Summary: 
[2022-07-14 23:28:19,924] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04090859182178974
[2022-07-14 23:28:28,633] INFO: Iter 24200 Summary: 
[2022-07-14 23:28:28,634] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04083812076598406
[2022-07-14 23:28:38,135] INFO: Iter 24300 Summary: 
[2022-07-14 23:28:38,135] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04078649561852217
[2022-07-14 23:28:47,318] INFO: Iter 24400 Summary: 
[2022-07-14 23:28:47,319] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04081921730190516
[2022-07-14 23:28:56,614] INFO: Iter 24500 Summary: 
[2022-07-14 23:28:56,615] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04067529816180468
[2022-07-14 23:29:19,160] INFO: Iter 24600 Summary: 
[2022-07-14 23:29:19,161] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04092761944979429
[2022-07-14 23:29:30,142] INFO: Iter 24700 Summary: 
[2022-07-14 23:29:30,143] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.041030113622546195
[2022-07-14 23:29:40,475] INFO: Iter 24800 Summary: 
[2022-07-14 23:29:40,476] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04074132289737463
[2022-07-14 23:29:49,829] INFO: Iter 24900 Summary: 
[2022-07-14 23:29:49,830] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04073281113058329
[2022-07-14 23:29:59,390] INFO: Iter 25000 Summary: 
[2022-07-14 23:29:59,391] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04085396822541952
[2022-07-14 23:30:31,446] INFO: Iter 25100 Summary: 
[2022-07-14 23:30:31,446] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040599988922476765
[2022-07-14 23:30:43,013] INFO: Iter 25200 Summary: 
[2022-07-14 23:30:43,014] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040940200574696065
[2022-07-14 23:30:53,875] INFO: Iter 25300 Summary: 
[2022-07-14 23:30:53,875] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04088467735797167
[2022-07-14 23:31:06,313] INFO: Iter 25400 Summary: 
[2022-07-14 23:31:06,314] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04087997522205114
[2022-07-14 23:31:15,994] INFO: Iter 25500 Summary: 
[2022-07-14 23:31:15,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040676222778856755
[2022-07-14 23:31:24,791] INFO: Iter 25600 Summary: 
[2022-07-14 23:31:24,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04094750590622425
[2022-07-14 23:31:33,411] INFO: Iter 25700 Summary: 
[2022-07-14 23:31:33,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040626411586999894
[2022-07-14 23:31:42,397] INFO: Iter 25800 Summary: 
[2022-07-14 23:31:42,398] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040729206055402756
[2022-07-14 23:31:51,150] INFO: Iter 25900 Summary: 
[2022-07-14 23:31:51,150] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040747795552015305
[2022-07-14 23:32:45,015] INFO: Iter 26000 Summary: 
[2022-07-14 23:32:45,015] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04072327680885792
[2022-07-14 23:33:02,115] INFO: Iter 26100 Summary: 
[2022-07-14 23:33:02,115] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04039794754236937
[2022-07-14 23:33:18,041] INFO: Iter 26200 Summary: 
[2022-07-14 23:33:18,042] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040610135197639466
[2022-07-14 23:33:27,271] INFO: Iter 26300 Summary: 
[2022-07-14 23:33:27,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0407841794192791
[2022-07-14 23:33:36,625] INFO: Iter 26400 Summary: 
[2022-07-14 23:33:36,626] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04027436722069979
[2022-07-14 23:33:45,937] INFO: Iter 26500 Summary: 
[2022-07-14 23:33:45,938] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040686944499611855
[2022-07-14 23:33:55,067] INFO: Iter 26600 Summary: 
[2022-07-14 23:33:55,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040791570693254474
[2022-07-14 23:34:04,428] INFO: Iter 26700 Summary: 
[2022-07-14 23:34:04,429] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04065250258892775
[2022-07-14 23:34:22,546] INFO: Iter 26800 Summary: 
[2022-07-14 23:34:22,547] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040525059401988986
[2022-07-14 23:34:32,342] INFO: Iter 26900 Summary: 
[2022-07-14 23:34:32,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04030000850558281
[2022-07-14 23:34:41,504] INFO: Iter 27000 Summary: 
[2022-07-14 23:34:41,504] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04077758502215147
[2022-07-14 23:34:50,840] INFO: Iter 27100 Summary: 
[2022-07-14 23:34:50,840] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040827248245477676
[2022-07-14 23:34:59,606] INFO: Iter 27200 Summary: 
[2022-07-14 23:34:59,606] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04066825069487095
[2022-07-14 23:35:08,613] INFO: Iter 27300 Summary: 
[2022-07-14 23:35:08,614] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04056738041341305
[2022-07-14 23:35:29,024] INFO: Iter 27400 Summary: 
[2022-07-14 23:35:29,025] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040450976602733134
[2022-07-14 23:35:38,108] INFO: Iter 27500 Summary: 
[2022-07-14 23:35:38,109] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040883384980261325
[2022-07-14 23:35:48,020] INFO: Iter 27600 Summary: 
[2022-07-14 23:35:48,020] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04054046858102083
[2022-07-14 23:35:57,271] INFO: Iter 27700 Summary: 
[2022-07-14 23:35:57,272] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040274480655789376
[2022-07-14 23:36:06,546] INFO: Iter 27800 Summary: 
[2022-07-14 23:36:06,547] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040671529583632945
[2022-07-14 23:36:15,648] INFO: Iter 27900 Summary: 
[2022-07-14 23:36:15,648] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04050175353884697
[2022-07-14 23:36:49,189] INFO: Iter 28000 Summary: 
[2022-07-14 23:36:49,189] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040801004059612754
[2022-07-14 23:37:04,176] INFO: Iter 28100 Summary: 
[2022-07-14 23:37:04,176] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04056151296943426
[2022-07-14 23:37:21,014] INFO: Iter 28200 Summary: 
[2022-07-14 23:37:21,015] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061751615256071
[2022-07-14 23:37:30,615] INFO: Iter 28300 Summary: 
[2022-07-14 23:37:30,615] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0406398406624794
[2022-07-14 23:37:39,843] INFO: Iter 28400 Summary: 
[2022-07-14 23:37:39,844] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04064210385084152
[2022-07-14 23:37:55,734] INFO: Iter 28500 Summary: 
[2022-07-14 23:37:55,734] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04029548272490501
[2022-07-14 23:38:05,176] INFO: Iter 28600 Summary: 
[2022-07-14 23:38:05,176] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040558545477688314
[2022-07-14 23:38:14,618] INFO: Iter 28700 Summary: 
[2022-07-14 23:38:14,618] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0405997783690691
[2022-07-14 23:38:24,048] INFO: Iter 28800 Summary: 
[2022-07-14 23:38:24,048] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04061796717345714
[2022-07-14 23:38:33,460] INFO: Iter 28900 Summary: 
[2022-07-14 23:38:33,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040529586635529996
[2022-07-14 23:38:49,924] INFO: Iter 29000 Summary: 
[2022-07-14 23:38:49,924] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040277461931109426
[2022-07-14 23:38:59,184] INFO: Iter 29100 Summary: 
[2022-07-14 23:38:59,184] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04063216157257557
[2022-07-14 23:39:07,995] INFO: Iter 29200 Summary: 
[2022-07-14 23:39:07,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04050163205713034
[2022-07-14 23:39:17,286] INFO: Iter 29300 Summary: 
[2022-07-14 23:39:17,286] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04043719585984945
[2022-07-14 23:39:26,499] INFO: Iter 29400 Summary: 
[2022-07-14 23:39:26,499] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04042163446545601
[2022-07-14 23:39:35,673] INFO: Iter 29500 Summary: 
[2022-07-14 23:39:35,673] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040526259653270245
[2022-07-14 23:39:51,678] INFO: Iter 29600 Summary: 
[2022-07-14 23:39:51,679] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.040458430647850034
[2022-07-14 23:40:01,394] INFO: Iter 29700 Summary: 
[2022-07-14 23:40:01,394] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0406836824119091
[2022-07-14 23:40:10,792] INFO: Iter 29800 Summary: 
[2022-07-14 23:40:10,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04043845266103745
[2022-07-14 23:40:20,178] INFO: Iter 29900 Summary: 
[2022-07-14 23:40:20,179] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04071509674191475
[2022-07-14 23:40:29,533] INFO: Iter 30000 Summary: 
[2022-07-14 23:40:29,534] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04021745808422565
[2022-07-14 23:40:40,956] INFO: Iter 30100 Summary: 
[2022-07-14 23:40:40,956] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.039751197546720504
[2022-07-14 23:40:53,315] INFO: Iter 30200 Summary: 
[2022-07-14 23:40:53,315] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03927027676254511
[2022-07-14 23:41:03,126] INFO: Iter 30300 Summary: 
[2022-07-14 23:41:03,127] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038993504904210566
[2022-07-14 23:41:12,330] INFO: Iter 30400 Summary: 
[2022-07-14 23:41:12,331] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03876279849559069
[2022-07-14 23:41:22,041] INFO: Iter 30500 Summary: 
[2022-07-14 23:41:22,041] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03937882114201784
[2022-07-14 23:41:31,314] INFO: Iter 30600 Summary: 
[2022-07-14 23:41:31,314] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03889442302286625
[2022-07-14 23:41:40,552] INFO: Iter 30700 Summary: 
[2022-07-14 23:41:40,553] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03882299955934286
[2022-07-14 23:42:02,374] INFO: Iter 30800 Summary: 
[2022-07-14 23:42:02,374] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03870443571358919
[2022-07-14 23:42:11,841] INFO: Iter 30900 Summary: 
[2022-07-14 23:42:11,842] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03850605584681034
[2022-07-14 23:42:21,650] INFO: Iter 31000 Summary: 
[2022-07-14 23:42:21,651] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038744141049683094
[2022-07-14 23:42:30,485] INFO: Iter 31100 Summary: 
[2022-07-14 23:42:30,485] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03893603183329106
[2022-07-14 23:42:39,099] INFO: Iter 31200 Summary: 
[2022-07-14 23:42:39,099] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03871546138077974
[2022-07-14 23:42:48,293] INFO: Iter 31300 Summary: 
[2022-07-14 23:42:48,293] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0386052269116044
[2022-07-14 23:43:07,156] INFO: Iter 31400 Summary: 
[2022-07-14 23:43:07,156] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03881802845746279
[2022-07-14 23:43:16,244] INFO: Iter 31500 Summary: 
[2022-07-14 23:43:16,245] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03844109565019607
[2022-07-14 23:43:25,363] INFO: Iter 31600 Summary: 
[2022-07-14 23:43:25,363] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038627342134714124
[2022-07-14 23:43:35,142] INFO: Iter 31700 Summary: 
[2022-07-14 23:43:35,142] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0386437813192606
[2022-07-14 23:43:47,321] INFO: Iter 31800 Summary: 
[2022-07-14 23:43:47,321] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03881697379052639
[2022-07-14 23:44:22,164] INFO: Iter 31900 Summary: 
[2022-07-14 23:44:22,165] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0383123791217804
[2022-07-14 23:44:39,653] INFO: Iter 32000 Summary: 
[2022-07-14 23:44:39,653] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038807159550487996
[2022-07-14 23:45:28,081] INFO: Iter 32100 Summary: 
[2022-07-14 23:45:28,082] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038591994680464266
[2022-07-14 23:45:43,893] INFO: Iter 32200 Summary: 
[2022-07-14 23:45:43,893] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03837781831622124
[2022-07-14 23:46:36,870] INFO: Iter 32300 Summary: 
[2022-07-14 23:46:36,871] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038552992083132266
[2022-07-14 23:47:20,773] INFO: Iter 32400 Summary: 
[2022-07-14 23:47:20,773] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038883030600845817
[2022-07-14 23:47:29,557] INFO: Iter 32500 Summary: 
[2022-07-14 23:47:29,557] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03847292993217707
[2022-07-14 23:47:38,874] INFO: Iter 32600 Summary: 
[2022-07-14 23:47:38,874] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03860153883695602
[2022-07-14 23:47:48,005] INFO: Iter 32700 Summary: 
[2022-07-14 23:47:48,005] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03845284942537546
[2022-07-14 23:47:57,400] INFO: Iter 32800 Summary: 
[2022-07-14 23:47:57,401] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038582844845950606
[2022-07-14 23:48:09,439] INFO: Iter 32900 Summary: 
[2022-07-14 23:48:09,440] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0386888886243105
[2022-07-14 23:48:18,735] INFO: Iter 33000 Summary: 
[2022-07-14 23:48:18,736] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0384239149838686
[2022-07-14 23:48:28,738] INFO: Iter 33100 Summary: 
[2022-07-14 23:48:28,739] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03812261328101158
[2022-07-14 23:48:37,868] INFO: Iter 33200 Summary: 
[2022-07-14 23:48:37,868] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038581590428948404
[2022-07-14 23:48:47,225] INFO: Iter 33300 Summary: 
[2022-07-14 23:48:47,226] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03856372080743313
[2022-07-14 23:48:56,089] INFO: Iter 33400 Summary: 
[2022-07-14 23:48:56,090] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038587807901203634
[2022-07-14 23:49:11,933] INFO: Iter 33500 Summary: 
[2022-07-14 23:49:11,934] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038152768537402154
[2022-07-14 23:49:21,007] INFO: Iter 33600 Summary: 
[2022-07-14 23:49:21,008] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03852456979453564
[2022-07-14 23:49:30,072] INFO: Iter 33700 Summary: 
[2022-07-14 23:49:30,072] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038550888150930405
[2022-07-14 23:49:39,601] INFO: Iter 33800 Summary: 
[2022-07-14 23:49:39,601] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03835835676640272
[2022-07-14 23:49:48,830] INFO: Iter 33900 Summary: 
[2022-07-14 23:49:48,830] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03872180312871933
[2022-07-14 23:50:04,816] INFO: Iter 34000 Summary: 
[2022-07-14 23:50:04,816] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03841351695358753
[2022-07-14 23:50:45,625] INFO: Iter 34100 Summary: 
[2022-07-14 23:50:45,625] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03849328149110079
[2022-07-14 23:50:59,210] INFO: Iter 34200 Summary: 
[2022-07-14 23:50:59,211] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038382913693785665
[2022-07-14 23:51:42,445] INFO: Iter 34300 Summary: 
[2022-07-14 23:51:42,446] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038036490045487883
[2022-07-14 23:51:57,421] INFO: Iter 34400 Summary: 
[2022-07-14 23:51:57,422] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03845137774944305
[2022-07-14 23:52:08,196] INFO: Iter 34500 Summary: 
[2022-07-14 23:52:08,197] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038461132273077965
[2022-07-14 23:52:42,941] INFO: Iter 34600 Summary: 
[2022-07-14 23:52:42,941] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03831490382552147
[2022-07-14 23:52:52,023] INFO: Iter 34700 Summary: 
[2022-07-14 23:52:52,024] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03826407950371504
[2022-07-14 23:53:00,828] INFO: Iter 34800 Summary: 
[2022-07-14 23:53:00,828] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03849722731858492
[2022-07-14 23:53:09,987] INFO: Iter 34900 Summary: 
[2022-07-14 23:53:09,988] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0382428727298975
[2022-07-14 23:53:27,784] INFO: Iter 35000 Summary: 
[2022-07-14 23:53:27,784] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03857382439076901
[2022-07-14 23:53:40,242] INFO: Iter 35100 Summary: 
[2022-07-14 23:53:40,243] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03818635124713182
[2022-07-14 23:53:49,194] INFO: Iter 35200 Summary: 
[2022-07-14 23:53:49,194] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038317138813436034
[2022-07-14 23:53:58,476] INFO: Iter 35300 Summary: 
[2022-07-14 23:53:58,477] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03875774335116148
[2022-07-14 23:54:07,461] INFO: Iter 35400 Summary: 
[2022-07-14 23:54:07,461] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03820563588291406
[2022-07-14 23:54:29,480] INFO: Iter 35500 Summary: 
[2022-07-14 23:54:29,481] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03824387263506651
[2022-07-14 23:54:43,146] INFO: Iter 35600 Summary: 
[2022-07-14 23:54:43,147] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038179674670100215
[2022-07-14 23:54:52,398] INFO: Iter 35700 Summary: 
[2022-07-14 23:54:52,398] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03831315163522959
[2022-07-14 23:55:01,975] INFO: Iter 35800 Summary: 
[2022-07-14 23:55:01,976] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03813739962875843
[2022-07-14 23:55:11,593] INFO: Iter 35900 Summary: 
[2022-07-14 23:55:11,593] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03841125715523958
[2022-07-14 23:55:20,650] INFO: Iter 36000 Summary: 
[2022-07-14 23:55:20,650] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038674209229648114
[2022-07-14 23:55:29,582] INFO: Iter 36100 Summary: 
[2022-07-14 23:55:29,582] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038088222704827784
[2022-07-14 23:55:44,257] INFO: Iter 36200 Summary: 
[2022-07-14 23:55:44,258] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03815359003841877
[2022-07-14 23:55:53,694] INFO: Iter 36300 Summary: 
[2022-07-14 23:55:53,694] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0383783707767725
[2022-07-14 23:56:02,976] INFO: Iter 36400 Summary: 
[2022-07-14 23:56:02,976] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03818061426281929
[2022-07-14 23:56:12,593] INFO: Iter 36500 Summary: 
[2022-07-14 23:56:12,593] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0383789960667491
[2022-07-14 23:56:21,675] INFO: Iter 36600 Summary: 
[2022-07-14 23:56:21,676] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03826743081212044
[2022-07-14 23:56:30,765] INFO: Iter 36700 Summary: 
[2022-07-14 23:56:30,765] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038303201384842396
[2022-07-14 23:56:47,567] INFO: Iter 36800 Summary: 
[2022-07-14 23:56:47,567] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03812700934708119
[2022-07-14 23:56:56,897] INFO: Iter 36900 Summary: 
[2022-07-14 23:56:56,898] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03826394394040108
[2022-07-14 23:57:06,160] INFO: Iter 37000 Summary: 
[2022-07-14 23:57:06,161] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03840384960174561
[2022-07-14 23:57:15,294] INFO: Iter 37100 Summary: 
[2022-07-14 23:57:15,295] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03806398954242468
[2022-07-14 23:57:25,148] INFO: Iter 37200 Summary: 
[2022-07-14 23:57:25,149] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0379974776878953
[2022-07-14 23:57:34,015] INFO: Iter 37300 Summary: 
[2022-07-14 23:57:34,015] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038110781349241736
[2022-07-14 23:57:52,080] INFO: Iter 37400 Summary: 
[2022-07-14 23:57:52,081] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03812118455767632
[2022-07-14 23:58:05,180] INFO: Iter 37500 Summary: 
[2022-07-14 23:58:05,181] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03812072034925222
[2022-07-14 23:58:15,956] INFO: Iter 37600 Summary: 
[2022-07-14 23:58:15,956] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0385488361492753
[2022-07-14 23:58:25,647] INFO: Iter 37700 Summary: 
[2022-07-14 23:58:25,647] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03861064422875643
[2022-07-14 23:58:34,863] INFO: Iter 37800 Summary: 
[2022-07-14 23:58:34,863] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038325271271169185
[2022-07-14 23:58:44,469] INFO: Iter 37900 Summary: 
[2022-07-14 23:58:44,471] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03815784964710474
[2022-07-14 23:59:00,634] INFO: Iter 38000 Summary: 
[2022-07-14 23:59:00,635] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03799300752580166
[2022-07-14 23:59:09,824] INFO: Iter 38100 Summary: 
[2022-07-14 23:59:09,824] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03810688957571983
[2022-07-14 23:59:19,022] INFO: Iter 38200 Summary: 
[2022-07-14 23:59:19,023] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03831707913428545
[2022-07-14 23:59:28,331] INFO: Iter 38300 Summary: 
[2022-07-14 23:59:28,332] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03815741907805204
[2022-07-14 23:59:37,566] INFO: Iter 38400 Summary: 
[2022-07-14 23:59:37,566] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038126504346728325
[2022-07-14 23:59:46,889] INFO: Iter 38500 Summary: 
[2022-07-14 23:59:46,890] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03786521803587675
[2022-07-14 23:59:56,331] INFO: Iter 38600 Summary: 
[2022-07-14 23:59:56,332] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03814975820481777
[2022-07-15 00:00:10,002] INFO: Iter 38700 Summary: 
[2022-07-15 00:00:10,002] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0381480173766613
[2022-07-15 00:00:19,371] INFO: Iter 38800 Summary: 
[2022-07-15 00:00:19,371] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0381257601454854
[2022-07-15 00:00:28,122] INFO: Iter 38900 Summary: 
[2022-07-15 00:00:28,123] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03833967346698046
[2022-07-15 00:00:37,049] INFO: Iter 39000 Summary: 
[2022-07-15 00:00:37,049] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038200533837080004
[2022-07-15 00:00:46,576] INFO: Iter 39100 Summary: 
[2022-07-15 00:00:46,576] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0379372688010335
[2022-07-15 00:00:55,508] INFO: Iter 39200 Summary: 
[2022-07-15 00:00:55,509] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0379277953505516
[2022-07-15 00:01:17,064] INFO: Iter 39300 Summary: 
[2022-07-15 00:01:17,065] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03819368124008179
[2022-07-15 00:01:26,178] INFO: Iter 39400 Summary: 
[2022-07-15 00:01:26,178] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03815219759941101
[2022-07-15 00:01:35,185] INFO: Iter 39500 Summary: 
[2022-07-15 00:01:35,186] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038311496414244174
[2022-07-15 00:01:44,458] INFO: Iter 39600 Summary: 
[2022-07-15 00:01:44,459] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03826261650770903
[2022-07-15 00:01:53,702] INFO: Iter 39700 Summary: 
[2022-07-15 00:01:53,703] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03822131376713514
[2022-07-15 00:02:02,787] INFO: Iter 39800 Summary: 
[2022-07-15 00:02:02,788] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03790209103375673
[2022-07-15 00:02:34,612] INFO: Iter 39900 Summary: 
[2022-07-15 00:02:34,613] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03825562942773104
[2022-07-15 00:02:44,731] INFO: Iter 40000 Summary: 
[2022-07-15 00:02:44,731] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.038161748498678205
